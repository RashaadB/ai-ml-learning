{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10129d00",
   "metadata": {},
   "source": [
    "## Introduction to Unsupervised Learning\n",
    "\n",
    "-  Unsupervised learning is a machine learning type where the model learns from unlabeled data to find patterns within it. \n",
    "    - deals with unlabeled datasets\n",
    "    - enables algorithms to work independently to discover hidden information or patterns in the data without guidance\n",
    "    - It classifies unsorted information according to patterns, differences, or similarities.\n",
    "        - Example: A data scientist inputs information about elderly people admitted to the hospital.\n",
    "            The algorithm receives no external input to influence categorization. \n",
    "\n",
    "- Unsupervised learning algorithms are used to detect the following in the data:\n",
    "    - Patterns\n",
    "    - Differences\n",
    "    - Similarities\n",
    "\n",
    "- Categorization of the given data can be based on age, average time spent in the hospital, and the types of diseases patients suffer from.\n",
    "- The final categorization of a dataset aitds in deriving conclusions based on different patterns generated from the data. \n",
    "- In unsupervised learning, the model derives insights from the data without being taught anything.\n",
    "\n",
    "    - It uncovers several previously unknown patterns in the dataset.\n",
    "    - It helps in finding features that may be useful for categorization.\n",
    "\n",
    "## Approaches to Unsupervised Algorithm\n",
    "\n",
    "- There are several approaches to unsupervised learning, each suited to different tasks:\n",
    "\n",
    "    - `Clustering`: a popular approach that groups data points into clusters based on their similarities. Cmmon clustering algorithms include k-means, hierarchical clustering, and density-based clustering like DBSCAN.\n",
    "\n",
    "    - `Dimensionality reduction`: This approach aims to reduce the number of features in a dataset while preserving the essential information. This can be useful for visualization and improving the efficiency of other machine learning algorithms. Principal component analysis (PCA) is a widely used dimensionality reduction technique. \n",
    "\n",
    "    - `Association rule learning`: This approach discovers relationships between different variables in a large dataset. It helps identify items that frequently appear together, which can be valuable for tasks like market basket analysis. The Apriori algorithm is a common example of this approach\n",
    "\n",
    "        -   Market basket analysis is a data mining technique used by retailers to discover relationships between items that people buy together frequently. It's often leveraged in retail sales to identify strong correlations between products. A famous example of market basket analysis is the \"diapers and beer\" story, where it was found that these two products were often purchased together, leading to marketing strategies that placed these items closer to encourage further sales.\n",
    "\n",
    "## Clustering Techniques\n",
    "\n",
    "- Clustering is a technique in unsupervised learning where data points are grouped together based on their similarities, aiming to discover inherent patterns or structures within the data.\n",
    "\n",
    "    - Clustering techniques divide a set of data points into multiple clusters, ensuring similarity within each cluster.\n",
    "    - Its goal is to segregate data points with similar traits.\n",
    "\n",
    "- the most common clustering algorithms used in unsupervised learning are:\n",
    "\n",
    "    - K-means clustering\n",
    "    - K-medoids\n",
    "    - Agglomerative clustering\n",
    "    - Density-based spatial clustering of applications with noise (DBSCAN)\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "- K-means clustering is an unsupervised machine learning algorithm that divides data into k clusters by minimizing the within-cluster variance.\n",
    "    - It groups unlabeled data into clusters by identifying the k numbers of centroids\n",
    "    - It assigns every data point to the closest cluster by calculating and using the pairwise Euclidean distance between points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026c771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation of K-Means clustering model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('Mall_customers.csv')\n",
    "df.head()\n",
    "\n",
    "# Extract the relevant features Annual Income(k$) and Spending Score (1-100) from the dataset\n",
    "X = df[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910ba1e",
   "metadata": {},
   "source": [
    "## Elbow method to find the optimal K\n",
    "- The elbow method involves plotting the number of clusters against the distortion or inertia to identify a significant flattening point, known as the elbow point. \n",
    "- The elbow point represents a trade-off between capturing meaningful patterns and avoiding excessive complexity, indicating the optimal numnber of clusters.\n",
    "- by choosing the value of k at the elbow point, you strike a balance between cluster quality and simplicity, resulting in a reasonable number of clusters. \n",
    "\n",
    "## steps to perform:\n",
    "\n",
    "-  Calculate the within-cluster sum of squares (WCSS) for different numbers of clusters.\n",
    "    - WCSS measures how compact a cluster is in k-means clustering. It calculates the total squared distance of all points within a cluster to their clusters centroid. \n",
    "    It tells you how spread out the points are within a cluster. \n",
    "    - The lower the WCSS, the closer the points are to their cluster's center. Plot the WCSS values to find the optimal number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2442f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    model = KMeans(n_clusters= i, n_init=10, init = 'k-means++', random_state=42)\n",
    "    model.fit(X)\n",
    "    wcss.append(model.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Observation\n",
    "# in the plotted graph, identify where the WCSS graph starts to flatten out. The plot flattens at 5. Hence this number is chosen as the Optimal k\n",
    "\n",
    "# Train the K-means model with the optimal number of clusters. \n",
    "model = KMeans(n_clusters = 5, n_init = 10, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = model.fit_predict(X)\n",
    "\n",
    "#Plot the clusters and their centroids on a scatter plot\n",
    "# Assign the color for each point. \n",
    "# make title, xlabel, and ylabel\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c65f5c",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "- K-means clusters with K = 5.\n",
    "\n",
    "\n",
    "- Cluster 1 (Red):\n",
    "\n",
    "    - Positioned at lower annual income and moderate to high spending scores.\n",
    "    This cluster might represent customers who, despite having lower incomes, tend to spend a significant portion of their income.\n",
    "\n",
    "- Cluster 2 (Blue):\n",
    "\n",
    "    - Middle to high annual income with the lowest spending scores among all clusters.\n",
    "    These customers are earning a good amount but are conservative in their spending habits.\n",
    "\n",
    "- Cluster 3 (Green):\n",
    "\n",
    "    - Lower income and lower spending scores.\n",
    "    Represents a conservative customer group with limited financial flexibility.\n",
    "\n",
    "- Cluster 4 (Cyan):\n",
    "\n",
    "    - Higher income and very high spending scores.\n",
    "    These are the premium customers who earn a lot and spend a lot, likely the target for high-end marketing campaigns.\n",
    "\n",
    "- Cluster 5 (Magenta):\n",
    "\n",
    "    - High income but moderate spending scores.\n",
    "    Customers in this cluster have high earning potential but do not spend as extravagantly as those in Cluster 4.\n",
    "\n",
    "## Silhouette Score\n",
    "\n",
    "- Silhouette score measures how well data points fit their assigned cluster by considering both similarity within a cluster and separation between clusters. \n",
    "- Measures the quality of clustering by comparing how similar an object is to its own cluster versus other clusters. \n",
    "\n",
    "    - Intra-cluster distance is the average distance between points within the dame cluster. A lower intra-cluster distance indicates that the cluster is more compact, which is generally desirable. \n",
    "    - Inter-cluster distance meaures the distance between clusters. Ideally, you want clusters to be as far apart as possible (high inter-cluster distance) to ensure that they are distinct from one another. \n",
    "\n",
    "- The score ranges from -1 to 1.\n",
    "\n",
    "    - Close to 1: indicates that the object is well-clustered and appropriately assigned to its cluster.\n",
    "    - Close to 0: indicates that the object lies on or very close to the boundary between two clusters.\n",
    "    - Close to -1: indicates that the object is poorly clustered and may have been assigned to the wrong cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement silhouette score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "df = pd.read_csv('Mall_customers.csv')\n",
    "X = df[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "\n",
    "# Silhouette Score method\n",
    "silhouette_scores = []\n",
    "for i in range(2, 11):\n",
    "    model = KMeans(n_clusters=i, n_init=10,  init='k-means++', random_state=42)\n",
    "    model.fit(X)\n",
    "    score = silhouette_score(X, model.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores)\n",
    "plt.title('Silhouette Score Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "#Observations \n",
    "#From the plot, the optimal number of clusters k can be chosen based on the highest Silhouette Score. \n",
    "# This score represents a balance between having clusters that are dense and well-separated from each other. Looking at the plot:\n",
    "# The Silhouette Score peaks at k=5. This suggests that the clusters are most distinct and appropriately separated when the data is divided into 5 clusters.\n",
    "\n",
    "# Select the number of clusters with the highest silhouette score\n",
    "optimal_clusters = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]\n",
    "\n",
    "# Fit the model with the optimal number of clusters\n",
    "model = KMeans(n_clusters=optimal_clusters, n_init=10, init='k-means++', random_state=42)\n",
    "y_kmeans = model.fit_predict(X)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')\n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f56329",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "- Hierarchical clustering is a method that groups data points based on their similarity or distance. It assumes that data points that are close to each other are more similar or related than those that are farther apart.\n",
    "\n",
    "- Hierarchical clustering, unlike K-Means, does not require specifying the number of clusters in advance. It builds a hierarchy of clusters based on the similarity or distance between data points, capturing complex and nested cluster shapes. This method provides a detailed view of data relationships through a dendrogram, making it more flexible for exploring the data's structure.\n",
    "\n",
    "- It adopts either of the following approaches for grouping data:\n",
    "\n",
    "    - Agglomerative Hierarchical Cluster Analysis: Bottom-to-top approach\n",
    "    - Divisive Hierarchical Cluster Analysis: Top-to-bottom approach\n",
    "\n",
    "## Dendrograms and Linkage Criteria in Hierarchical Clustering\n",
    "\n",
    "-  `Dendrograms`: A dendrogram is a tree-like diagram that displays the relationships between similar objects. Each branch represents a category or class, and the entire diagram shows the hierarchical structure connecting all the categories or classes. \n",
    "\n",
    "- Comonents of a Dendrogram:\n",
    "\n",
    "    - `Leaves (Terminal Nodes)`: These represent the individual data points at the bottom of the dendrogram. \n",
    "    - `Branches (Internal Nodes)`: These represent the clusters formed by merging or splitting. The height of the branches indicates the distance or dissimilarity between clusters.\n",
    "    - `Height`: The vertical axis of the dendrogram represents the distance or dissimilarity at which clusters are merged or split. A greater height indicates a higher dissimilarity.\n",
    "\n",
    "## How to read a Dendrogram\n",
    "\n",
    "- `Merging`: Starting from the leaves, data points that are close to each other are merged first, forming small clusters. \n",
    "- `Splitting`: As you move up, these small clusters are further merged based on their distance or dissimilarity until all data points are combined into a single cluster at the root. \n",
    "- `Cluster Formation`: By setting a threshold on the height, you can determine the number of clusters. Horizontal cuts across the dendrogram at a specific height level show the clusters formed at the dissimilarity level. \n",
    "\n",
    "- `Linkage Criteria`: It determines how the distance between clusters is calculated during the merging process. Different linkage methods can result in different clustering outcomes. \n",
    "\n",
    "## Common Linage Criteria\n",
    "\n",
    "- `Single Linkage (minimum Linkage)`: The distance between two clusters is defined as the minimum distance between any sinle data point in one cluster and any single data point in the other cluster. It can lead to chaining effects, where clusters can form long, elongated shapes. \n",
    "\n",
    "- `complete linkage (maximum linkage)`: The distance between two clusters is defined as the maximum distance between any single data point in one cluster and any single data point in the other cluster. It tends to create more compact and spherical clusters.\n",
    "\n",
    "- `Average Linkage`: The distance between two clusters is defined as the average distance between all pairs of data points, one from each cluster. It provides a balance between single and complete linkage.\n",
    "\n",
    "- `Centroid Linkage`: The distance between two clusters is defined as the distance between the centroids (mean points) of the clusters. It can be sensitive to the shapes of the clusters.\n",
    "\n",
    "\n",
    "## Working of Hierarchical Clustering\n",
    "\n",
    "- Consider a dataset of n different types of animals.\n",
    "    - Assume that each animal is a distinct cluster by itself, that is, n clusters.\n",
    "    - Take the two closest data points and make them into a cluster. Now, there are n-1 clusters.\n",
    "    - Repeat the process as mammals are grouped into one cluster, reptiles into another, fish into a third cluster, and so on.\n",
    "    - Group mammals, reptiles, and fish into the vertebrate cluster and insects, corals, and arachnids into the invertebrate cluster.\n",
    "\n",
    "- Hierarchical clustering is the result of the of the creation of a tree-shaped structure known as a dendrogram.\n",
    "\n",
    "    - A dendrogram is a visual interpretation of the hierarchical connections between items.\n",
    "    - The goal is to find the best approach to assigning items to a cluster.\n",
    "\n",
    "- To choose the number of clusters to be created:\n",
    "    - Identify the longest line that traverses the maximum vertical distance without intersecting any of the merging points in the dendrogram.\n",
    "    - Draw a horizontal line where the line can traverse the maximum vertical distance without intersecting the merging point.\n",
    "    - The number of vertical lines it intersects is the optimal number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement hierarchical clustering\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('Mall_customers.csv')\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "# there are no null values or missing values\n",
    "\n",
    "#create a df1\n",
    "# Choosing the first 50 rows from df to create a new dataframe df1 with relevant features\n",
    "# annual income and spending score\n",
    "df1 = df.iloc[:51, 3:5]\n",
    "\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.title(\"Customer Dendograms\")\n",
    "dend = shc.dendrogram(shc.linkage(df1, method='ward'))\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Distance between clusters')\n",
    "\n",
    "#Observations:\n",
    "# see the dendrogram for customer data.\n",
    "# There are different clusters that have been created.\n",
    "# Blue represents one cluster; orange represents one cluster; and the entire green represents one whole cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5136248",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical Cluster Analysis: Bottom-to-Top Approach\n",
    "\n",
    "- Agglomerative hierarchical cluster analysis or the bottom-up approach, creates a more informative structure than flat clustering. This method doesn't require specifying the number of clusters beforehand. It starts with each data point as its own cluster and progressively merges pairs of clusters until all data points are combined into a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406492ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\n",
    "labels_=model.fit_predict(df1)\n",
    "\n",
    "# Create scatter plot of the dataset\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(df1.iloc[:,0], df1.iloc[:,1], c=model.labels_, cmap='rainbow')\n",
    "plt.title('Clusters of Customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "\n",
    "#Observations\n",
    "# Within the spread, you can see that five separate clusters have been created, forming the agglomerative cluster of five clusters.\n",
    "# The cluster is represented by red, green, blue, violet, and yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd852dc",
   "metadata": {},
   "source": [
    "## Divisive Hierarchical Cluster Analysis: Top-to-Bottom Approach\n",
    "\n",
    "- This method, also called top-down clustering, begins with all data points in one large cluster. It then repeatedly splits this cluster into smaller sub-clusters based on their differences.\n",
    "\n",
    "## Steps of Divisiive Clustering\n",
    "\n",
    "1. Start with All Data in One Cluster: Begin with a single cluster containing all data points.\n",
    "2. Select a Cluster to Split: Choose a cluster to split based on some criteria (e.g., the largest cluster or the one with the highest variance).\n",
    "3. Split the Cluster: Divide the selected cluster into two smaller clusters using a clustering algorithm (e.g., k-means with $𝑘=2$).\n",
    "4. Repeat: Repeat steps 2 and 3 until a stopping criterion is met (e.g., a specified number of clusters is reached or the clusters cannot be split further).\n",
    "\n",
    "## DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "- DBSCAN is a popular unsupervised machine learning algorithm primarily used for clustering tasks, where the goal is to group closely packed data points based on some notion of distance. It identifies points that are alone in low-density regions as outliers or noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e62968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('Mall_customers.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Select relevant features\n",
    "X = data[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "X_train, X_test = train_test_split(X, test_size=0.4, random_state=42)\n",
    "\n",
    "# Apply DBSCAN to training and test set\n",
    "db = DBSCAN(eps=5, min_samples=5).fit(X_train)\n",
    "labels_train = db.labels_\n",
    "\n",
    "# Apply DBSCAN to the test set\n",
    "labels_test = db.fit_predict(X_test)\n",
    "\n",
    "#Calculate the number of clusters in the training and test set\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_train = len(set(labels_train)) - (1 if -1 in labels_train else 0)\n",
    "n_noise_train = list(labels_train).count(-1)\n",
    "\n",
    "n_clusters_test = len(set(labels_test)) - (1 if -1 in labels_test else 0)\n",
    "n_noise_test = list(labels_test).count(-1)\n",
    "\n",
    "print(f'Estimated number of clusters (train): {n_clusters_train}')\n",
    "print(f'Estimated number of noise points (train): {n_noise_train}')\n",
    "print(f'Estimated number of clusters (test): {n_clusters_test}')\n",
    "print(f'Estimated number of noise points (test): {n_noise_test}')\n",
    "\n",
    "# Plot the clusters for training set\n",
    "unique_labels_train = set(labels_train)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_train))]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for k, col in zip(unique_labels_train, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels_train == k)\n",
    "\n",
    "    xy = X_train[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title(f'Training Set: Estimated number of clusters: {n_clusters_train}')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.show()\n",
    "\n",
    "# Plot the clusters for test set\n",
    "unique_labels_test = set(labels_test)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_test))]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for k, col in zip(unique_labels_test, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels_test == k)\n",
    "\n",
    "    xy = X_test[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title(f'Test Set: Estimated number of clusters: {n_clusters_test}')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563e4ce",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "- The DBSCAN clustering algorithm identified 2 clusters in both the training and test sets, with a significant number of noise points (black dots) spread across the entire feature space. Cluster 1 is positioned around an annual income of 60k and a spending score of 60, while Cluster 2 is centered around an annual income of 60k and a spending score of 40. The presence of these two clusters in both datasets indicates that the clustering is consistent, though the majority of data points are classified as noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
