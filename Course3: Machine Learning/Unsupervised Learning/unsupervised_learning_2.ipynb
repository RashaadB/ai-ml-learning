{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5806d2",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Techniques \n",
    "\n",
    "- Dimensionality reduction involves decreasing the number of features (or dimensions) in a dataset while preserving as much information as possible. This technique is used for various purposes, such as simplifying a model, enhancing the performance of a learning algorithm, or making the data easier to visualize.\n",
    "\n",
    "## Importance of Dimensionality Reduction\n",
    "\n",
    "1. **Improves computational efficiency:** Reduces the computational cost for data processing and model training.\n",
    "2. **Mitigates the curse of dimensionality:** Simplifies data to prevent overfitting and sparsity issues.\n",
    "3. **Reduces noise in data:** Eliminates irrelevant or noisy features to enhance model performance.\n",
    "4. **Enhances data visualization:** Makes high-dimensional data easier to visualize in 2D or 3D.\n",
    "5. **Boosts model performance:** Focuses on the most relevant features for better accuracy.\n",
    "6. **Saves storage and memory:** Decreases the amount of storage and memory needed for large datasets.\n",
    "7. **Increases model interpretability:** Simplifies models, making them easier to understand and explain.\n",
    "8. **Avoids multicollinearity:** Addresses high correlation between features to improve regression models.\n",
    "\n",
    "## Approaches to Dimensionality Reduction\n",
    "\n",
    "- There are two ways to apply the dimension reduction technique, Feature Selection, Feature Extraction. \n",
    "\n",
    "- `Feature Selection`:\n",
    "\n",
    "    - Feature selection is the process of choosing a subset of relevant features and discarding irrelevant ones from a dataset to build a more accurate model. Only want to keep optimal features for the input data.\n",
    "\n",
    "- Three methods are used for the feature selection:\n",
    "\n",
    "- **Filter Methods:** this method involves filtering the dataset to retain only the relevant features.\n",
    "\n",
    "  Common techniques include: Correlation, Chi-Square Test, and ANOVA (these techniques are already covered in ADSP course).\n",
    "\n",
    "- **Wrapper Methods:** this method evaluates subsets of features using a machine learning model. Features are added or removed based on their impact on model performance. It is more accurate but also more complex than filter methods.\n",
    "\n",
    "  Common techniques include: Forward Selection and Backward Selection.\n",
    "\n",
    "- **Embedded Methods:** these methods evaluate the importance of features during the training process of the machine learning model.\n",
    "\n",
    "  Common techniques include: LASSO, Elastic Net, and Ridge Regression (these techniques are covered in detail in regression lesson).\n",
    "\n",
    "\n",
    "`Feature Extraction`:\n",
    "\n",
    "- Feature extraction is the process of transforming high-dimensional data into a lower-dimensional space. This approach is useful for retaining essential information while using fewer resources for processing. \n",
    "\n",
    "Some common feature extraction techniques are:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Independent Component Analysis (ICA)\n",
    "\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- Principal Component Analysis (PCA) is a statistical technique for dimensionality reduction in data analysis. It simplifies the complexity of high-dimensional data while preserving trends and patterns.\n",
    "\n",
    "- Real-world problems typically deal with datasets that have a huge number of features. \n",
    "\n",
    "- Example: High-resolution images that need classification or power allocation exercises across multiple communication channels that have high dimensionality. Dealing with such datasets demands increased computational power and more complex algorithms.\n",
    "\n",
    "- Principal Component Analysis (PCA) is an unsupervised learning technique used to preprocess datasets and reduce their dimensionality while preserving the original dataset. \n",
    "\n",
    "## Common Terms in PCA \n",
    "\n",
    "- Dimensionality: It is the number of features present in the data.\n",
    "\n",
    "- Correlation: It indicates the strength of the relationship between features. The correlation value ranges between -1 and +1. It is -1 when variables are inversely proportional and +1 when they are directly proportional.\n",
    "\n",
    "- Orthogonality: Dimensionality reduction techniques often utilize orthogonality to maintain the independence of features when reducing the number of dimensions in a dataset.\n",
    "\n",
    "- Covariance Matrix: It is a matrix containing the covariance between variables.\n",
    "\n",
    "- Variance is a measure of the variability or spread of a single variable, indicating how much the values differ from the mean.\n",
    "\n",
    "- Eigenvector: Given a square matrix  ùê¥  and a nonzero vector  ùë£ , and  ùë£  is the eigenvector if  ùê¥ùë£  (the result of applying matrix  ùê¥  to  ùë£ ) is a scalar multiple of  ùë£ , i.e.  ùê¥ùë£  =  ùúÜùë£ where  ùúÜ  is the eigenvalue\n",
    "\n",
    "- Eigenvalues: The scalar  ùúÜ  associated with the eigenvector  ùë£  in the transformation  ùê¥ùë£  =  ùúÜùë£ , indicating how much the eigenvector is scaled during the transformation. It represents the variance captured by each principal component, indicating their importance in explaining the data's variability.\n",
    "\n",
    "- Principal component: Principal components are new variables created as linear combinations of the original variables, arranged to be uncorrelated and to compress most of the information into the initial components. In this way, from 10-dimensional data, PCA aims to maximize the information in the first component, then the next most in the second, and so on.\n",
    "\n",
    "## Steps involved in PCA \n",
    "\n",
    "- Standardization\n",
    "- Covariance matrix computation\n",
    "- Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "- Create a Feature vector\n",
    "- Recasting data along the principal component axes\n",
    "\n",
    "## Standardization \n",
    "- Standardization adjusts the range of variables so that each one contributes equally, ensuring uniformity in their influence. \n",
    "    - Standardization transforms data by rescaling it to have mean of 0 and a standard deviation of 1, ensuring consistent ranges and making it more suitable for comparison and analysis across different variables. \n",
    "    - This normalization process mitigates the dominance of variables with larger ranges over those with smaller values.\n",
    "\n",
    "## Covariance Matrix Computation\n",
    "- It helps to check the correlation between features in a dataset.\n",
    "\n",
    "    ## types of Covariance:\n",
    "    - Positive covariance indicates a direct correlation.\n",
    "    - Negative covariance indicates an inverse correlation. \n",
    "\n",
    "- The covariance matrix provides a summary of the relationships (correlations) between variables in a tabular representation. \n",
    "\n",
    "## Identifying Principal Components\n",
    "\n",
    "- Eigenvectors and eigenvalues, computed from the covariance matrix, determine the principal components of data. Each eigenvector, paired with a corresponding eigenvalue, represents an axis direction where the data variance is maximized-these are the principal components. The eigenvalues indicate the amount of variance each component carries. By ordering the eigenvectors from highest to lowest eigenvalues, you rank the principal components by their significance.\n",
    "\n",
    "## Create a feature vector\n",
    "\n",
    " - Decide whether to retain all components or to discard the less significatn ones (those with lower eigenvalues). Then a matrix is formed from the remaining, more significant eigenvectors, knows as the feature vector. \n",
    "\n",
    "- the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions.\n",
    "\n",
    "## Recasting Data Along Principal Component Axes\n",
    "\n",
    "- In this step aim to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components. \n",
    "\n",
    "## Application of PCA\n",
    "\n",
    "- PCA compresses information into a smaller set with new dimensions. \n",
    "\n",
    "    - In neuroscience, it identifies the action potential of neurons by their shape.\n",
    "    - In quantitative finance, it redueces the complexity of stocks analysis. \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements PCA\n",
    "\n",
    "#This dataset consists of 2000 samples with 8 features: preg, plas, pres, skin, insu, mass, pedi, and age. \n",
    "# Each sample includes a target variable class, which indicates whether the sample tested positive or negative for a condition.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head()\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = data.drop(columns=['class'])\n",
    "y = data['class'].apply(lambda x: 1 if x == 'tested_positive' else 0)\n",
    "\n",
    "# the dataset is split into training (60%) and testing (40%) sets. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# apply the PCA\n",
    "\n",
    "# Import PCA from sklearn.decomposition.\n",
    "# Find the optimal number of principal components\n",
    "# Instantiate a PCA object with the optimal number of components.\n",
    "# Fit PCA to the scaled data.\n",
    "# Transform the scaled data using PCA.\n",
    "\n",
    "# Apply PCA without reducing dimensionality to find the optimal number of components\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# Explained variance ratios\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center',\n",
    "        label='Individual explained variance')\n",
    "plt.step(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), where='mid',\n",
    "         label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.title('Explained Variance by Different Principal Components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Observations\n",
    "# The plot shows that each of the 8 principal components explains a similar amount of variance individually, \n",
    "# while the cumulative explained variance steadily increases, approaching 100% by the 8th component.\n",
    "\n",
    "# Eigenvalues (which are proportional to the explained variance)\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='-', label='Eigenvalues')\n",
    "plt.xlabel('Principal components')\n",
    "plt.ylabel('Eigenvalues')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Observation \n",
    "# The scree plot shows the eigenvalues, representing the explained variance ratio, for each of the eight principal components. \n",
    "# The eigenvalues decrease sharply from the 1st to the 2nd principal component, continue to decline at a slower rate up to the 3rd component, and then begin to level off from the 4th component onwards. \n",
    "# This pattern indicates that the first few components capture most of the variance in the data, \n",
    "# with diminishing returns for each additional component beyond the 3rd or 4th. Thus, the plot suggests that the optimal number of principal components is around 3 or 4.\n",
    "\n",
    "import pandas as pd\n",
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "#create dataframe\n",
    "df_train_pca = pd.DataFrame(data=X_train_pca, columns=['Principal Component 1', 'Principal Component 2', 'Principal Component 3'])\n",
    "df_train_pca['Target'] = y_train\n",
    "\n",
    "df_test_pca = pd.DataFrame(data=X_test_pca, columns=['Principal Component 1', 'Principal Component 2', 'Principal Component 3'])\n",
    "df_test_pca['Target'] = y_test\n",
    "\n",
    "#visualize the results for training and testing set. \n",
    "# Plotting the 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Training set\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "colors = ['r', 'b']\n",
    "for target, color in zip([0, 1], colors):\n",
    "    indices = df_train_pca['Target'] == target\n",
    "    ax1.scatter(df_train_pca.loc[indices, 'Principal Component 1'],\n",
    "                df_train_pca.loc[indices, 'Principal Component 2'],\n",
    "                df_train_pca.loc[indices, 'Principal Component 3'],\n",
    "                c=color, s=50)\n",
    "ax1.set_xlabel('Principal Component 1')\n",
    "ax1.set_ylabel('Principal Component 2')\n",
    "ax1.set_zlabel('Principal Component 3')\n",
    "ax1.legend(['No Diabetes', 'Diabetes'])\n",
    "ax1.set_title('PCA of Diabetes Dataset (Training set)')\n",
    "ax1.grid()\n",
    "\n",
    "# Testing set\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "for target, color in zip([0, 1], colors):\n",
    "    indices = df_test_pca['Target'] == target\n",
    "    ax2.scatter(df_test_pca.loc[indices, 'Principal Component 1'],\n",
    "                df_test_pca.loc[indices, 'Principal Component 2'],\n",
    "                df_test_pca.loc[indices, 'Principal Component 3'],\n",
    "                c=color, s=50)\n",
    "ax2.set_xlabel('Principal Component 1')\n",
    "ax2.set_ylabel('Principal Component 2')\n",
    "ax2.set_zlabel('Principal Component 3')\n",
    "ax2.legend(['No Diabetes', 'Diabetes'])\n",
    "ax2.set_title('PCA of Diabetes Dataset (Testing set)')\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d49ff",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Linear discriminant analysis (LDA) is a technique used for dimensionality reduction and classification. It aims to project the data onto a lower-dimensional space in such a way that the separation between different classes is maximized. LDA focuses on finding a linear combination of features that best separate two or more classes of objects or events.\n",
    "\n",
    "- LDA assumes the data follows a Gaussian distribution.\n",
    "- It assumes that the covariance matrices of different classes are equal.\n",
    "- It assumes the data is linearly separable, allowing for an accurate linear decision boundary to classify different classes.\n",
    "- It can reduce the dimensionality of the data to a maximum of  ùëò‚àí1  components, where  ùëò  is the number of classes in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb241aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement LDA\n",
    "\n",
    "# Instantiate a LDA object with one components.\n",
    "# Fit LDA to the scaled data.\n",
    "# Transform the scaled data using LDA.\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=1)\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "#Create DataFrames\n",
    "df_train_lda = pd.DataFrame(data=X_train_lda, columns=['LDA Component 1'])\n",
    "df_train_lda['Target'] = y_train\n",
    "\n",
    "df_test_lda = pd.DataFrame(data=X_test_lda, columns=['LDA Component 1'])\n",
    "df_test_lda['Target'] = y_test\n",
    "\n",
    "# Visualize the results for traning and testing set\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['r', 'b']\n",
    "for target, color in zip([0, 1], colors):\n",
    "    indices = df_train_lda['Target'] == target\n",
    "    plt.scatter(df_train_lda.loc[indices, 'LDA Component 1'], np.zeros_like(df_train_lda.loc[indices, 'LDA Component 1']),\n",
    "                c=color, s=50)\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('Constant zero line')\n",
    "plt.legend(['No Diabetes', 'Diabetes'])\n",
    "plt.title('LDA of Diabetes Dataset (Training set)')\n",
    "plt.grid()\n",
    "\n",
    "# Testing set\n",
    "plt.subplot(1, 2, 2)\n",
    "for target, color in zip([0, 1], colors):\n",
    "    indices = df_test_lda['Target'] == target\n",
    "    plt.scatter(df_test_lda.loc[indices, 'LDA Component 1'], np.zeros_like(df_test_lda.loc[indices, 'LDA Component 1']),\n",
    "                c=color, s=50)\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('Constant zero line')\n",
    "plt.legend(['No Diabetes', 'Diabetes'])\n",
    "plt.title('LDA of Diabetes Dataset (Testing set)')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Observation\n",
    "# In both plots, the LDA component is plotted along the x-axis. \n",
    "# The points are colored according to their target labels: red for non diabetes and blue for diabetes. \n",
    "# The zero y-values are used to clearly separate the data points for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0834f1d",
   "metadata": {},
   "source": [
    "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    " - t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction algorithm that uses a randomized approach to non-linearly reduce the dimensionality of a dataset. It focuses on retaining the local structure of the data in the lower-dimensional space.\n",
    "\n",
    " - This algorithm helps explore high-dimensional data by mapping it into lower dimensions while preserving local relationships. As a result, we can visualize and understand the structure of the dataset by plotting it in 2D or 3D.\n",
    "\n",
    "- The MNIST dataset is loaded. It contains 60,000 training images and 10,000 test images of handwritten digits (0-9).\n",
    "Here we are taking y as label to plot the visualization, it is not used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mnist.csv')\n",
    "\n",
    "# Pixel values\n",
    "X = df.iloc[:, 1:].values  \n",
    "\n",
    "#Labels\n",
    "y = df.iloc[:, 0].values  \n",
    "\n",
    "df.head()\n",
    "\n",
    "# Applying t-SNE\n",
    "# TSNE from sklearn.manifold is used to reduce the data to 2 dimensions for visualization.\n",
    "# Only a subset of 1000 samples is used for performance reasons (t-SNE can be computationally intensive).\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=42, n_iter=1000)\n",
    "\n",
    "tsne_data = model.fit_transform(X[:1000])\n",
    "\n",
    "# Creating a new DataFrame to help us in plotting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, y[:1000])).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
    "\n",
    "# Plotting the result of t-SNE\n",
    "sns.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, \"Dim_1\", \"Dim_2\").add_legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399ad2b",
   "metadata": {},
   "source": [
    "## Association Rule Learning\n",
    "\n",
    "- Association Rule Learning is a popular unsupervised learning technique used to uncover relationships, patterns, or associations among a set of items in large datasets. This technique is commonly used in market basket analysis, where the goal is to identify sets of products that frequently co-occur in transactions.\n",
    "\n",
    "- The two key concepts in association rule learning are frequent itemsets and association rules.\n",
    "\n",
    "- Frequent Itemsets:\n",
    "\n",
    "    - These are groups of items that appear frequently together in transactions.\n",
    "    - The frequency is measured by the support count, which is the number of transactions containing the itemset.\n",
    "\n",
    "- Association Rules:\n",
    "\n",
    "    - These are implications of the form  ùê¥,ùêµ‚Üíùê∂ , meaning that if items A and B are bought, then item C is likely to be bought.\n",
    "\n",
    "    - Support: The proportion of transactions that contain the itemset or how frequently an item appears in the dataset.\n",
    "\n",
    "    - Confidence: The probability that a transaction containing the antecedent also contains the consequent or how often the rule has been found to be true.\n",
    "\n",
    "    - Lift: The ratio of the observed support to that expected if the items were independent. A lift greater than 1 indicates a positive association.\n",
    "\n",
    "## Practical Application: \n",
    "\n",
    "1. **Market Basket Analysis:** Identifying products that are frequently bought together to optimize product placement and promotions.\n",
    "2. **Web Usage Mining:** Analyzing user navigation patterns to improve website design and content recommendation.\n",
    "3. **Bioinformatics:** Discovering relationships between genes and proteins.\n",
    "4. **Fraud Detection:** Identifying patterns in fraudulent transactions.\n",
    "\n",
    "## Popular Algorithms:\n",
    "\n",
    "**Apriori Algorithm:**\n",
    "- It uses breadth-first search and Hash Tree to calculate the itemset efficiently.\n",
    "- Generates frequent itemsets by iteratively expanding smaller itemsets.\n",
    "- Uses the _Apriori Property_ which states that all non-empty subsets of a frequent itemset must also be frequent.\n",
    "\n",
    "**Eclat Algorithm:**\n",
    "\n",
    "- Uses a depth-first search strategy to find frequent itemsets.\n",
    "- It is more efficient for dense datasets.\n",
    "\n",
    "## Apriori Algorithm \n",
    "\n",
    "- The Apriori algorithm is a classic algorithm used for mining frequent itemsets and learning association rules over transactional databases. It is an unsupervised learning technique, typically used in market basket analysis to find interesting relationships between items in large datasets.\n",
    "\n",
    "- The algorithm operates by identifying the frequent individual items in the database and extending them to larger itemsets as long as those itemsets appear sufficiently often in the database.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv', header=None)\n",
    "\n",
    "# Convert the DataFrame to a list of lists\n",
    "transactions = []\n",
    "for i in range(data.shape[0]):\n",
    "    transactions.append([str(data.values[i, j]) for j in range(data.shape[1]) if str(data.values[i, j]) != 'nan'])\n",
    "\n",
    "# Transaction Encoding: \n",
    "# use the TransactionEncoder from the mlxtend.preprocessing module to convert the list of lists into a one-hot encoded DataFrame. \n",
    "# In this format, each column represents an item, and each row represents a transaction, with binary values indicating whether an item was purchased in that transaction.\n",
    "\n",
    "# Initialize the TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Display the first few rows of the one-hot encoded DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# applying the apriori algorithm: \n",
    "\n",
    "# The apriori function from the mlxtend.frequent_patterns module was used to find frequent itemsets. We specified a minimum support threshold of 0.01 (1%), meaning that an itemset must appear in at least 1% of transactions to be considered frequent.\n",
    "# The result is a DataFrame where each row represents a frequent itemset, and the columns provide the support (proportion of transactions containing the itemset) and the itemsets themselves.\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Apply the Apriori algorithm with a minimum support of 0.01 (1%)\n",
    "frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "frequent_itemsets.head(5)\n",
    "\n",
    "#Rules\n",
    "# The association_rules function was used to generate association rules from the frequent itemsets. We specified a minimum confidence threshold of 0.2 (20%), \n",
    "# meaning that the rules must have a confidence of at least 20% to be considered.\n",
    "# The result is a DataFrame where each row represents an association rule, and the columns provide various metrics related to the rule.\n",
    "\n",
    "# Generate the association rules with a minimum confidence of 0.2 (20%)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2)\n",
    "\n",
    "# Display the results\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head())\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca677d",
   "metadata": {},
   "source": [
    "## Eclat Algorithm\n",
    "\n",
    "- The ECLAT algorithm, which stands for Equivalence Class Clustering and bottom-up Lattice Traversal, is a widely-used method for Association Rule mining. It is considered more efficient and scalable than the Apriori algorithm.\n",
    "- While Apriori operates in a horizontal fashion similar to Breadth-First Search in a graph, ECLAT functions vertically, akin to Depth-First Search. This vertical approach makes ECLAT faster than Apriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyECLAT import ECLAT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Market_Basket_Optimisation.csv', header=None)\n",
    "\n",
    "# Convert the DataFrame to a list of lists\n",
    "transactions = []\n",
    "for i in range(data.shape[0]):\n",
    "    transactions.append([str(data.values[i, j]) for j in range(data.shape[1]) if str(data.values[i, j]) != 'nan'])\n",
    "    \n",
    "    \n",
    "# Split the transactions into training and testing sets\n",
    "train_transactions, test_transactions = train_test_split(transactions, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataFrames from the list of lists for training and testing sets\n",
    "train_df = pd.DataFrame(train_transactions)\n",
    "test_df = pd.DataFrame(test_transactions)\n",
    "\n",
    "\n",
    "\n",
    "# Perform ECLAT algorithm using pyECLAT on the training set\n",
    "eclat_instance = ECLAT(data=train_df, verbose=True)\n",
    "\n",
    "# Get the frequent itemsets with a minimum support of 0.01 (1%) on the training set\n",
    "support_dict, frequent_itemsets = eclat_instance.fit(min_support=0.01, min_combination=1, max_combination=2)\n",
    "\n",
    "# Convert frequent itemsets to a DataFrame for better readability\n",
    "total_transactions = len(transactions)\n",
    "\n",
    "frequent_itemsets_df = pd.DataFrame({\n",
    "    'Itemset': list(frequent_itemsets.keys()),\n",
    "    'Support': [len(support_dict[item]) / total_transactions for item in frequent_itemsets.keys()]\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by 'Support' in descending order\n",
    "frequent_itemsets_df_sorted = frequent_itemsets_df.sort_values(by='Support', ascending=False)\n",
    "\n",
    "frequent_itemsets_df_sorted.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf7dd6",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "**Mineral Water (0.193574):** The highest support value in this snippet, indicating that mineral water appears in about 19.36% of all transactions. This suggests it's a very popular item among customers.\n",
    "\n",
    "**Eggs (0.142781):** Also showing high popularity, eggs are included in approximately 14.28% of transactions.\n",
    "\n",
    "**Spaghetti (0.139315), French Fries (0.135715), and Chocolate (0.130516):** These items are also commonly purchased, each appearing in about 13-14% of transactions, reflecting their strong customer demand.\n",
    "\n",
    "**Cooking Oil & Mineral Water (0.016798), Meatballs (0.016798), Almonds (0.016798):** These itemsets show a much lower support, appearing in about 1.68% of transactions. The combination of cooking oil and mineral water might indicate a specific usage pattern or a niche but relevant market segment.\n",
    "\n",
    "\n",
    "\n",
    "## Anomaly Detection Techniques \n",
    "\n",
    "- Anomaly detection is a technique used to identify rare items, events, or outliers that differ significantly from the majority of the data. In unsupervised learning, anomaly detection is particularly challenging because there are no labeled examples of anomalies to guide the learning process.\n",
    "\n",
    "## Isolation forest\n",
    "\n",
    "- Isolation Forest is an unsupervised learning algorithm for anomaly detection that works by isolating observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The key idea is that anomalies are few and different, so they are more susceptible to isolation.\n",
    "\n",
    "- **Unique Approach:** Isolation Forest does not rely on proximity measures like traditional methods.\n",
    "- **Random Feature Selection:** It randomly selects features and splits them at random values.\n",
    "- **Isolation Process:** This process creates partitions or \"trees\" to isolate individual data points.\n",
    "- **Anomaly Detection:** Anomalies, being fewer and further from the norm, typically require fewer splits to isolate.\n",
    "- **Efficiency:** This makes anomalies easier and faster to detect compared to normal observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9928b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'credit_card_fraud.csv'  # Update the path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Features for training\n",
    "features = ['V1', 'V2', 'V3', 'V4', 'V5', 'Amount']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df['Class'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training features:\", X_train.columns)\n",
    "print(\"Testing features:\", X_test.columns)\n",
    "\n",
    "# Fit the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "\n",
    "# Predict anomalies (-1 for anomalies, 1 for normal points) on the training set\n",
    "train_anomaly_predictions = iso_forest.predict(X_train)\n",
    "train_anomaly_scores = iso_forest.decision_function(X_train)\n",
    "\n",
    "# Predict anomalies on the test set\n",
    "test_anomaly_predictions = iso_forest.predict(X_test)\n",
    "test_anomaly_scores = iso_forest.decision_function(X_test)\n",
    "\n",
    "# Add predictions and scores to the test set\n",
    "X_test['Anomaly'] = test_anomaly_predictions\n",
    "X_test['Anomaly Score'] = test_anomaly_scores\n",
    "\n",
    "# Evaluate the results on the test set\n",
    "print(X_test['Anomaly'].value_counts())\n",
    "\n",
    "# Plot the results for the test set\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_test['Amount'], X_test['Anomaly Score'], c=y_test, cmap='coolwarm')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Isolation Forest Anomaly Detection (Credit Card Fraud) on Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c157c09",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In conclusion, unsupervised learning techniques are powerful tools for exploring and extracting meaningful insights from unlabeled data. By allowing algorithms to autonomously identify patterns and structures within datasets, you can uncover hidden relationships, detect anomalies, and gain a deeper understanding of the underlying data distribution. From clustering and dimensionality reduction to association rule learning, the applications of unsupervised learning are vast and diverse, spanning fields such as data analysis, pattern recognition, and anomaly detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
