 Welcome to the Machine learning folder, hopefuly things are starting to come together as why we focused on python refreshers and learning and reviewing math and stats.
 In this folder we learn about all of the componets for Machine Learning starting with types of Supervised learning types: Classical and Regressions. And learning about all of the algorithms used to predict outcomes based on labeled datasets.

## Supervised Regression Learning Folder##

## 1. Supervised Learning

`supervised_learning.ipynb`

-    Introduction to Supervised Learning
-    Supervised Learning Categories
-    Classification
-    Regression
-    Applications of Supervised Learning
-    Supervised Learning algorithm
-    Linear Regression
-    Logistic Regression
-    Na√Øve Bayes
-    K- Nearest Neighbors (KNN)
-    Decision Trees
-    Random Forests
-    Support Vector Machines (SVM)


## 2. Supervised Learning: Regression

`supervised_learning_regression.ipynb`

Types of Regression
-  Linear Regression
-  Simple Linear Regression
-  Train- Test Split
-  Multiple Linear Regression
-  Overfitting and Underfitting
-  Non- Linear Regression
-  Polynomial Regression
-  Model Evaluation and Validation
-  Cross- Validation Techniques
-  Performance Metrics for Regression
-  Mean Squared Error (MSE)
-  Root Mean Squared Error (RMSE)
-  Mean Absolute Error (MAE)
-  R- Squared
-  Regularization Techniques
-  Lasso Regression
-  Ridge Regression
-  ElasticNet Regression
-  Hyperparameter Tuning
-  GridSearchCV
-  RandomSearchCV

## Supervised Classification Learning Folder##

`supervised_learning_classification.ipynb`
-  Classification
-  Applications of Classification
-  Types of Classification
-  Binary Classification
-  Logistic Regression
-  Mathematical Concept of Logistic Regression
-  Example with Breast Cancer Dataset
-  Performance Metrics Used in Classification
-  Significance of the Confusion Matrix
-  Naive Bayes Classifier
-  Mathematical Concept of Naive Bayes
-  Applying Naive Bayes Algorithm on Breast Cancer Dataset
-  K- Nearest Neighbors (KNN)
-  Applying K- Nearest Neighbors on Breast Cancer Dataset
-  Hyperparameter Tuning in KNN
-  Decision Tree
-  How Decision Trees Work
-  Metrics for Splitting
-  Pruning
-  Applying Decision Tree on Breast Cancer Dataset
-  Hyperparameter Tuning in Decision Tree
-  Support Vector Machine (SVM)
-  Applying SVM on Breast Cancer Dataset
-  Hyperparameter Tuning

## Supervised Classification part 2
`classification_2.ipynb`

-  Multiclass Classification
-  Example with Online Gaming Behavior Dataset
-  Naive Bayes Algorithm
-  K- Nearest Neighbors
-  Decision Tree
-  Random Forest
-  Multi- Label Classification
-  Examples of Multi- label classification
-  Algorithms for Multi- Label Classification
-  Challenges in Multi- Label Classification

## Supervised Classification part 3
`classification_3.ipynb`

-  Handling Imbalanced Data in Classification
-  Introduction to Imbalanced Data
-  Oversampling Techniques (e.g., SMOTE)
-  Undersampling Techniques
-  Ensemble Methods for Imbalanced Data

## Ensemble Learning Folder

`ensemble_learning.ipynb`

-  Introduction to ensemble learning
-  Goals of ensemble learning
-  Importance of ensemble learning
-  Weak and Strong learners in Ensemble learning
-  Categories in ensemble learning
-  Sequential ensemble technique
-  Parallel ensemble technique
-  Simple techniques used in ensemble learning
-  Voting
-  Hard Voting
-  Soft Voting
-  Averaging
-  Weighted Averaging
-  Advanced techniques used in ensemble learning
-  Bagging (bootstrap aggregating)
-  Bagging Techniques
-  Advantages of bagging
-  Disadvantages of bagging
-  Out- of- bag (OOB) concept
-  Boosting
-  Boosting Techniques
-  Advantages of boosting
-  Disadvantages of boosting
-  Stacking
-  Advantages of stacking
-  Disadvantages of stacking

## Unsupervised Learning Folder

`unsupervised_learning.ipynb`

-  Introduction to Unsupervised Learning
-  What Is Unsupervised Learning?
-  Approaches to Unsupervised algorithm (Clustering, Dimensionality Reduction, Association rule )
-  Clustering Techniques
-  Overview of Clustering
-  K- Means Clustering:
-  Algorithm and Implementation
-  Choosing the Number of Clusters (Elbow Method, Silhouette Score)
-  Hierarchical Clustering:
-  Agglomerative vs. Divisive Methods
-  Dendrograms and Linkage Criteria
-  DBSCAN (Density- Based Spatial Clustering of Applications with Noise)

## Unsupervised Learning part 2

`unsupervised_learning_2.ipynb`

-  Dimensionality Reduction Techniques:
-  Importance of Dimensionality Reduction
-  Principal Component Analysis (PCA)
-  Linear Discriminant Analysis (LDA)
-  t- Distributed Stochastic Neighbor Embedding (t- SNE)
-  Association Rule Learning
-  Introduction to Association Rule Learning
-  Apriori Algorithm
-  Eclat Algorithm
-  Anomaly Detection Techniques
-  Isolation forest
-  Model Evaluation in Unsupervised Learning:
-  Silhouette Score for Clustering


## Recommendation Systems Folder 

`recommendation_systems.ipynb`

- Overview of Recommendation Systems
- What are Recommendation Systems?
- Importance and Applications

- Examples of Recommendation Systems
- Enhanced Book Discovery Recommendations
- Hyper- Personalized Media Recommendations
- Viewed Items Recommendations
- Enhanced Product Discovery Recommendations

- Types of Recommendation Systems
- Collaborative Filtering
- Memory- Based Collaborative Filtering
- User- Based Collaborative Filtering
- Item- Based Collaborative Filtering
- Model- Based Collaborative Filtering
- Matrix Factorization (MF)
- Singular Value Decomposition (SVD)
- Content- Based Filtering
- Hybrid Filtering

- Advanced Techniques in Recommendation Systems
- The GetTopN Function
- Hit Rate

- Addressing Challenges in Recommendation Systems
- Cold Start Problem
- Implicit and Explicit Feedback