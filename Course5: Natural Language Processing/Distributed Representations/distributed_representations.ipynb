{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e32f8b6",
   "metadata": {},
   "source": [
    "## Drawbacks of Fundamental Vectorization Apporaches\n",
    "- Basic vectorization methods like BoWs and TF-IDF employ discrete representations\n",
    "- These methods treat words as independent entities, ignoring the nuances and associations.\n",
    "- This limits their capacity to capture the relationships between words impacting its overall understanding of the language\n",
    "- The features vectors are sparse and high dimensional, with dimensionality growing along with the vocabulary size\n",
    "- most values in the vectors are zero, impeding learning capability and causing computational inefficiency\n",
    "\n",
    "## Distributed Representations\n",
    "- Distributed representations refer to method of representing words or entities as vectors in a multi-dimensional space\n",
    "- They leverage neural network architectures to generate compact, low-dimensional representations for words and texts\n",
    "\n",
    "## Word Embeddings\n",
    "- Word embedding is a technique that represents words as dense vectors in a continous space, capturing semantic relationships and contextual meanings. \n",
    "\n",
    "## Word2Vec\n",
    "- Word2Vec is a word embedding technique in NLP that represents words as continuous vectors in a way that captures semantic relationships based on their contextual usage\n",
    "- is a two layer neural network, where input is text corpus and output is set of vectors\n",
    "- Word2Vec has two main variants:\n",
    "\n",
    "    - `Continuous bag of words`: the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears.\n",
    "        - Context size refers to the number of surrounding words taken into consideration when predicting a target word\n",
    "\n",
    "    - `Skip gram`: the task is to predict the context words from the center word\n",
    "\n",
    "## GloVe Word Representation\n",
    "- Global Vectors for Word Representation (GloVe) is a word embedding model focussing on capturing global relationships between words based on their co-occurence probabilities in the entire corpus. \n",
    "\n",
    "- GloVe extreacts the co-occurence probability with the global statistics whereas word2vec is trained on the dataset it is made to focus on. \n",
    "\n",
    "- GloVe operates on the principle that the ratio of word-word co-occurrence probability can easily determine some meaning of the words to be vectorized. \n",
    "\n",
    "## FastText\n",
    "- FastText was developed by Facebook's AI research lab and is known for its efficiency, especially in handling out of vocabulary words and capturing subword information. \n",
    "\n",
    "## Doc2Vec\n",
    "- Doc2Vec captures the semantic meaning of entire documents as continuous vector representations. It was introduced by researchers at Google in 2014.\n",
    "- Doc2Vec shares a common architecture with the word2Vec model.\n",
    "- learns a paragraph vector to capture a representation for the entire text, considering words in context. \n",
    "\n",
    "    - Distributed memory version of paragraph vector (PV-DM) functions as a memory, noting whats missing or representing the paragraph's topic\n",
    "    \n",
    "    - Distributed bag of words version of paragraph vector (PV-DBOW) predicts words within a paragraph based solely on the paragraph content, without taking word order into account\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
