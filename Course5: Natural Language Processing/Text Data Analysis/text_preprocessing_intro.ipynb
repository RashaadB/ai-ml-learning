{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b47f08b",
   "metadata": {},
   "source": [
    "## NLP pipeline\n",
    "- NLP pipeline is a sequence of processing steps that are applied to a text document to extract useful information and insights. \n",
    "    - `Data Acquisition`: involves the systematic collection of textual information from various sources. \n",
    "    - text data can be acquired using Web scrapping, application programming interfaces (APIs), PDF to text, Data augmentation, image to text.\n",
    "       \n",
    "        - `Web Scrapping`: involves extracting text data from websites. in python web scraping is facilitated through libraries like BeautifulSoup, Scrapy\n",
    "\n",
    "        - `APIs`: provides structured access to information from various sources. \n",
    "            - APIs enable developers to programmatically retrieve textual infomration from various sources. \n",
    "            - online services like twitter and google maps offer such APIs for retrieval textual information from these platforms. \n",
    "\n",
    "        - `Image to Text`: computer vision and optical character recognition (OCR) are used to extract text content present in images. \n",
    "\n",
    "        - `PDF to Text`: PDF is a common format for storing textual data due to its versatility and ability to maintain document formatting. \n",
    "            - Many documents, including reports, articles, and manuals, are often distributed or archived in PDF format. \n",
    "            - In python there are various libraries that can be used to extract text from PDF files, which include PyMuPDF2, pdfminer.six, PyPDFium\n",
    "        - `Data Augmentation`: Data augmentation refers to the process of artificially expanding a dataset by generating diverse variations of existing textual data. \n",
    "            - these techniques aim to improve the robustness and generalization of machine learning models.\n",
    "            - like text synonym replacement, back translation, random swap\n",
    "\n",
    "## Text cleaning and text processing\n",
    "- like any other type of data source text data also requires cleaning and preprocessing. \n",
    "\n",
    "- `Tokenization`: involves segmenting unstructured data and natural language text into distinct units or elements, shaping the foundation for further analysis, breaking it down into individual works or tokens. \n",
    "\n",
    "- `Text cleaning`: The text data contains a lot of noise. These entities dont contribute any value to the analysis and therefore should be removed from the text. \n",
    "    - `Stop words`: refer to common words that are often removed furing text preprocessing to focus more on meaningful terms. common stop words: the, and, is, in, it, for, to, of, with, that. \n",
    "    - `punctuation`: punctuation marks like commas, periods, exclamation marks, question marks, semicolons, colons are often eliminated to simplify the text and reduce noise. \n",
    "    - `special characters`: @#$%^ are often removed to reduce noise and simplify the text for analysis. \n",
    "\n",
    "- `Stemming and Lemmatization`: serve to simplify text by reducing words to their fundamental forms, aiding in the reduction of vocabulary size. \n",
    "    - `Stemming`: refers to a straightforward approach that shortens word endings with the goal of achieving accurate results most of the time, often involving the removal of derivational affixes. \n",
    "        - example: jumps to jump, or Flying to fly, or Running to Run\n",
    "    \n",
    "    -  `Lemmatiazion`: involves reducing words to their basic form by utilizing vocabulary and morphological analysis, with the goal of returning the base or dictionary form of a word, referred as lemma. \n",
    "\n",
    "- `Parts of speech tagging`: provides essential information about the syntactic and semantic characteristics of words in any given text, enabling more accurate language understanding and processing. \n",
    "    - involves assigning words in a text corpus to specific parts of speech based on their definitions and contextual usage. \n",
    "\n",
    "- `Named entity recognition`: a detective for finding and sorting important things in text. \n",
    "It looks for specific types of things, such as names of organizations, people, places\n",
    "\n",
    "## vocabulary and feature extraction\n",
    "- in NLP vocabulary and feature textraction are fundamental aspects of transforming raw text data into a fomrat suitable for machine learning models. \n",
    "\n",
    "    - `Vocabulary`: comprises the set of distinct words found in a corpus or a collection of texts. It is formed by identifying unique tokens througout the entire corpus. \n",
    "    \n",
    "    - `Feature Extraction`: text data is unstructured, necessitating conversion into a numerical format for use in the machine learning models. \n",
    "        - `Fundamental sparse matrix`\n",
    "        - `positive and negative frequencies`\n",
    "\n",
    "## Sentiment Analysis in Python\n",
    "- Sentiment analysis, known as opinion mining, is the process of determining and extracting the sentiment or emotional tone expressed in a piece of text, such as a review, social media post or a comment. The goal is to understand whether the expressed sentiment is positive, negative, or neutral. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
