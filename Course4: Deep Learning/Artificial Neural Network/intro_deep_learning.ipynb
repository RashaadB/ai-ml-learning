{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c462180",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "- Using deep neural networks to extract patterns from unstructured and structured data, including images, text, audio, and video, to identify patterns and make accurate predictions\n",
    "\n",
    "## Deep Learning vs Machine Learning\n",
    "- `Deep Learning`: Subset of machine learning that focuses on training deep neural networks.\n",
    "handles unstructured data such as images, audio, text, and video. Reduces manual feature engineering by learning features on its own.\n",
    "\n",
    "- image recognition, natural language processing, and speech synthesis, requires alot of computational resources and large labeled datasets. deep neural networks with multiple layers and requires substantial computational resources. \n",
    "\n",
    "- requires high-quality GPUs with RAM are crucial for handling large datasets and performing extensive computations which is more expensive than machine learning. \n",
    "\n",
    "- `Machine Learning`:training algorithms to make predicitons or decisions based on data. Works with structured and unstructrued data. performance is dependent on the features engineered.\n",
    "\n",
    "- utilizes techniques like decision trees, support vector machines, and random forests\n",
    "\n",
    "- requires simple CPU for running simple machine learning models and data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb3ffe",
   "metadata": {},
   "source": [
    "## Applications of Deep Learning \n",
    "\n",
    "- Ai leverages deep learning within Natural Language Processing(NLP) to enhance human language interaction.\n",
    "- Autonomous self-driving cars use computer vision for object detection and classification\n",
    "- Automatic colorization of black and white images. \n",
    "- Deep learning and audio processing. Transforming speech into text with accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd86f2",
   "metadata": {},
   "source": [
    "## Limitations of Deep Learning\n",
    "\n",
    "- large amounts of data is required, collected, prepared and labeled depending on the task. \n",
    "-  expensive hardware is necessary to support the compute requriements, like high speed ram, GPUs.\n",
    "- deep learning models are not immune to overfitting and can be susceptible to it, this can be due to unavaiability of sufficient training data. \n",
    "- deep learning modles are difficult to explain, the models function are like black boxes, with known inputs and outputs. And not ideal for rigorous verification due to limited interpretability and reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d563e",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks\n",
    "- deep learning frameworks provide interfaces, libraries, and tools to facilitate the development of deep learning models. \n",
    "- frameworks provide foundations for designing, training, debugging and deploying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336aa7ca",
   "metadata": {},
   "source": [
    "## Learning FrameWorks\n",
    "- Deep learning frameworks: `Keras`, `TensorFlow`, `PyTorch`\n",
    "\n",
    "- `Keras`: open-source python frameworks for building and training deep neural networks with a user-friendly modular interface. Interface for TensorFlow library that is easy to use and simplistic. \n",
    "\n",
    "- `TensorFlow`: end to end open source platform for Machine learning and deep learning, develped by google and coded using python. Specifically designed for training and inference of neural networks. \n",
    "\n",
    "- `PyTorch`: Meta Ai developed and maintains, open source deep learning framework based on torch library. competitor to TensorFlow, and offers powerful GPU acceleration, and uses tape-based automatic differentiation system that computes the numerical derivative of a function defined by a computer program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b849f8",
   "metadata": {},
   "source": [
    "## Deep Learning Lifecycle\n",
    "\n",
    "- Planning phase\n",
    "- Data collection and labeling phase\n",
    "- Training phase\n",
    "- testing phase\n",
    "- Deployment\n",
    "- Model monitoring and Maintenance\n",
    "\n",
    "- `Planning phase`: process documentation for project, goals are set, and resources for the projects are defined\n",
    "\n",
    "- `Data collection and labeling phase`: setting up data capturing devices and deciding how to label the collected data. if collecting or labeling data is challenging, project goes back to the planning phase to make or come up with a more effective approach. \n",
    "\n",
    "- `Training phase`: Deep learning training, the model learns from data by adjusting its parameters through forward and backward propagation, minnimizing errors and enahancing predictions. \n",
    "- the chosen models are implemented and debugged, and the models performance gets tested, and continuous improvements are done until requirements are fulfilled.\n",
    "- not enough or bad data can lead to overfitting or data that isnt labeled right can cause the model to be inaccurate or unreliable\n",
    "\n",
    "- `testing phase`: evaluate the traineed model on unseen data and analyze the outputs for improvements. tests are conducted on the modle to compare the results, making adjustments or fixes to restore the performance to desired level. \n",
    "\n",
    "- `Deploying phase`: model is deployed for real time use, the goal is functionality\n",
    "\n",
    "- `Model monitoring and maintenance`: continuously check the models performance as it interacts with real-world data.\n",
    "- monitor model efficacy and ready to retrain the model with new datasets or adjust the model parameters if the is a performance issue. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
