{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487c6b71",
   "metadata": {},
   "source": [
    "## intro to PyTorch\n",
    "\n",
    "- PyTorch is an open-source deep learning framework, known for its capabilities in computer vision, natural language processing, and reinforcement learning. \n",
    "\n",
    "- More industries are adopting PyTorch for production due to tools like PyTorch Lighting and TorchServe, enhancing flexibility and performance.\n",
    "\n",
    "\n",
    "## Features of PyTorch\n",
    "- Dynamic computation graphs\n",
    "- Pythonic nature\n",
    "- community support for reasearch\n",
    "- integration with python libraries\n",
    "\n",
    "- torchvision: provides datasets, model architectures, and common image transformations for computer vision applications.\n",
    "\n",
    "- torechtext: it offers data processing utilities and popular datasets for natural language processing (NLP). \n",
    "\n",
    "- PyG: PyTorch Geometric enables learning on graphs and other irregular structures, enabling applications like social network analysis and knowledge graph completion.\n",
    "\n",
    "- torchaudio: It offers methods for audio processing and access to popular audio datasets, that are useful for speech recognition and audio classification. \n",
    "\n",
    "## PyTorch Tensors\n",
    "- Tensors in PyTorch resemble NumPy's ndarrays. A tensor represents a multidimensional array composed of elements of a single data type. \n",
    "\n",
    "## Modules in PyTorch\n",
    "- Each type of module in PyTorch serves a specific role in building and training nueral network models. These roles range from structuring the network architecture to optimiing its parameters during the training process. \n",
    "\n",
    "    ## Basic Layer Modules\n",
    "- `Linear Layers (nn.Linear)`: Fully connected linear layers\n",
    "\n",
    "- `Convolutional Layers (nn.Conv1d, nn.Conv2d, nn.Conv3d)`: layers that apply a convolution operation over incoming data, trypically used in image processing\n",
    "\n",
    "- `Recurrent Layers (nn.LSTM, nn.GRU, nn.RNN)`: layers designed for sequential data processing\n",
    "\n",
    "    ## Activation Functions\n",
    "- `ReLU (nn.ReLU)`: A common activation function that outputs the input directly if it is positive, otherwise it outputs zero\n",
    "\n",
    "- `Sigmoid (nn.Sigmoid), Tanh (nn.Tanh), LeakyReLU` are other activation functions that can be used. \n",
    "\n",
    "    ## Pooling Layers\n",
    "- Reduce the spatial dimensions of the input, which decreases the computational complexity and controls overfitting\n",
    "\n",
    "- `MaxPooling (nn.MaxPool1d, nn.AvgPool2d)`: Computes the average of elements in a region of the input. \n",
    "\n",
    "    ## Normalization Layers\n",
    "- Are used to stabilize the learning process by normalizing the input\n",
    "\n",
    "- `Batch Normalization (nn.BatchNorm1d, nn.BatchNorm2d)`: Normalizes the input to have zero mean and unit variabnce acoss the batch \n",
    "\n",
    "- `Layer Normalization (nn.LayerNorm)`: Normalizes input across the features instead of the batch. \n",
    "\n",
    "    ## Dropout Layers \n",
    "- Dropout is a regularization technique to prevent overfitting by randomly dropping units during the training process. \n",
    "\n",
    "- `Dropout (nn.Dropout)`: It randomly zeroes some of the elements of the input tensor with a given probability\n",
    "\n",
    "    ## Utility and Container Modules\n",
    "- These are used to organize or combine other modules, facilitating model contruction and management. \n",
    "\n",
    "- `Sequential (nn.Sequential)`: It is a simple sequential container to concatenate multiple modules\n",
    "\n",
    "- `ModuleList (nn.ModuleList)`: it holds submodules in a list\n",
    "\n",
    "- `ModuleDict (nn.ModuleDict)`: it holds submodules in a dictionary. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
