{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee011d6",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "## Sequential Modeling\n",
    "- Sequential modeling is a task or problem domain that involves predicting or modeling patterns in sequential data.\n",
    "\n",
    "- Types of sequential modeling\n",
    "\n",
    "    - RNNs\n",
    "    - Autoencoders\n",
    "    - Seq2Seq\n",
    "\n",
    "- To achieve memory based learning, artificial intelligence algorithm known as recurrent neural network is used. \n",
    "- The memor aspect of RNNs refers to their ability to use information from previous steps or time points, in the sequence to inform their current output. This is particularly useful in applications where the context is essential. \n",
    "\n",
    "    - Example: \n",
    "        An email spam classifier enhances user experience by identifying and filtering out unwanted or harmful emails, keeping the inbox organized and secure\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "- RNNs were introduced after feed forward neural networks (FNNs)\n",
    "- Is the first algorithm that remembers its input owing to internal memory\n",
    "- The ability to add the recent past to the present allows it to make precise predictions\n",
    "- ideal for machine learning problems that involvo sequential data, like natural language processing and time series forecasting.\n",
    "\n",
    "## Types of RNNs\n",
    "- `one-to-one`: one to one architecture forms the basis of feedforward neural networks. Example: image classification of handwritten digits\n",
    "    - Activation values are not needed due to straightforward scenario of a single input and a single output. \n",
    "\n",
    "- `one-to-many`: A sequence of multiple inputs is given to the network, which is instructed to predict a single output. Example: sentiment analysis where a sequence of words representing a positive or a negative emotion.\n",
    "\n",
    "- `Many-to-many`: A sequence of mulitple inputs is fed to the network to predict a sequence of outputs that may or may not be of the same length.\n",
    "these neural networks have two components: encoders and decoders. \n",
    "\n",
    "## RNN architecture\n",
    "- The recurrent neural network architecture is designed to process sequential data by capturing temporal deendencies and retaining the memory of previous inputs\n",
    "\n",
    "- input layer\n",
    "- hiddin layer with recurrent connections\n",
    "- output layer\n",
    "\n",
    "1. Initialization\n",
    "2. Input processing\n",
    "3. Hidden state update\n",
    "4. output calculation\n",
    "5. Training\n",
    "\n",
    "- `Initialization`: the hidden state is set to a vector of zeros or random values, and the RNN architecture is defined, including the number of hidden units, and activation functions. \n",
    "\n",
    "- `Input processing`: the input sequence is processed one element at a time\n",
    "    The current input is combined with the previous hidden state to capture the context and dependencies, and the activation of the hidden layer is computed based on the combined input. \n",
    "\n",
    "- `Hidden state update`: The hidden state is then updated using the computed activation of the hidden layer, allowing information to flow across different time steps. \n",
    "\n",
    "- `Output calculation`: The output of the network is calculated based on the updated hidden state, and any necessary transformations or activation functions are applied to obtain the desired output format\n",
    "\n",
    "- `Training`: The RNN calculates the loss between the predicted output and the target output, and the weights and biases of the RNN are updated using backpropagation through time and an optimization algorithm. This process helps the RNN learn how to make more accurate predictions and improve its performance. \n",
    "\n",
    "## Long short term memory (LSTM)\n",
    "- When back propagation, recurrent neural networks have issues with exploding and disappearing gradients. \n",
    "- The modified versions of RNNs that help address the exploding and vanishing gradient problem are long short term memory networks (LSTMs) and gated recurrent Units\n",
    "- Long short term memory is an RNN architedture designed to capture and retain long-term dependencies in sequential data, overcoming the limitations of traditional RNNs. \n",
    "- LSTM networks are widely used in tasks such as natural language processing, speech recognition, time series analysis, and more. \n",
    "\n",
    "- LSTM utilizes three types of gates to control the flow of information within the network. \n",
    "    - Forget gate: determines which information to discard from the previous memeory cell state\n",
    "    - input gate: regulates which new information to incorporate into the current memeory cell state\n",
    "    - output gate: controls the output generated by the memory cell\n",
    "\n",
    "## Gated Recurrent Network (GRU)\n",
    "- GRUs are type of RNN architecture that incorporate gating mechanisms to regulate the flow of information between neural network cells. \n",
    "\n",
    "## components of GRU\n",
    "- The update gate, like an LSTMs input and forget gate, determines whether the information is retained or discared. \n",
    "- the reset gate allows you to control the relevance of the previous cell state by deciding the extent to which past data should be ignored. \n",
    "\n",
    "## Hybrid Modeling\n",
    "- hybrid modeling is the practice of employing two different neural network models and merging them to achieve a sequence of tasks. \n",
    "    - a well know implementation is the concept of convolutional recurrent neural networks (CRNNs)\n",
    "- The recurrent network is used for temporal analysis, such as finding links between the extradcted features that influence the output. By merging the two networks, the machine can learn patterns in the sequential data provided and form appropriate predictions. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
