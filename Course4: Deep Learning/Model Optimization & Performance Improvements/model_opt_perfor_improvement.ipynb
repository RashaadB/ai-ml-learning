{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91c8a82",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- Minimizing or maximizing any mathematical expression involves finding the values that result in the lowest or highest outcome. This makes the predictions as accurate and optimized as possible.\n",
    "\n",
    "- the parameters of the model are adjusted and changed during the training phase to minimize the loss function. \n",
    "\n",
    "    - Example: Building bridges involves balancing weight capacity, cost, safety, and material use. This equilibrium is referred to as optimization. \n",
    "\n",
    "        - Aim for maximum load capacity given the design and usage\n",
    "        - Minimize material usage without compromising integrity\n",
    "        - Prioritize cost-effectiveness\n",
    "        - Uphold stringent safety standards\n",
    "\n",
    "## Optimization Algorithms\n",
    "- To minimize the loss function, optimization algorithms iteratively change the model's parameters during training. In deep learning, optimization algorithms optimize the cost function J. \n",
    "\n",
    "## Importance of Optimization Algorithms\n",
    "- Scalability\n",
    "- Efficiency\n",
    "- Problem solving\n",
    "- perofrmance improvements\n",
    "- Decision making\n",
    "- Resource allocation\n",
    "\n",
    "## Optimizer\n",
    "- Its the algorithms or methods that reduce losses by changing neural network attributes such as weights and learning rates. \n",
    "- Optimizers establish a connection between the loss function and model parameters, modifying the model based on the loss function's output. \n",
    "- they manipulate the neural network weights, modling and refining the model to achieve the highest possible accuracy. \n",
    "\n",
    "    - Example of Optimizer\n",
    "        - the loss unction serves as a roadmap for for the optimizer to indicate if it has taken the correct or wrong path. \n",
    "        - consider hikers attempting to descend a mountain with a blindfold. \n",
    "        - The hikers can't determine which way to travel but can tell if they are going down (making progress) or up (losing progress)\n",
    "        - If the hikers continue downhill, they will eventually reach the bottom. \n",
    "\n",
    "\n",
    "## Optimizers and their Types\n",
    "- Gradient Descent\n",
    "- Stochastic Gradient Descent\n",
    "- Momentum\n",
    "- AdaDelta\n",
    "- RMSprop\n",
    "- Adam\n",
    "- AdaGrad\n",
    "\n",
    "## Gradient Descent\n",
    "- Gradient descent is a typical optimization algorithm and the foundation for training deep learning models. Gradient descent iteratively adjusts the parameters in the direction that minimizes the loss function, gradually improving the model's fit to the data. \n",
    "\n",
    "- This technique involves calculating how much the model's error changes with its changes in parameters.\n",
    "- It modifies those parameters in a way that will drastically decrease the error.\n",
    "- The optimal weights for a model are not known before training but can be found through trial and error using the loss function. \n",
    "\n",
    "- Goal: The goal of gradient descent is to reduce the difference between what the model predicts and the actual outcomes. To achieve this, it uses learning rate and direction. \n",
    "\n",
    "- Gradient descent uses direction to gradually arrive at the local or global minimum (the point of convergence)\n",
    "\n",
    "- the dataset is shuffled for each iteration to avoid biasing the descent and improve the randomess of the mini-batch selection process. \n",
    "\n",
    "## Stochastic Gradient Descent \n",
    "- Stochastic gradient descent (SGD) updates parameters by evaluating the loss and gradient on mini-batches of data. It enables efficient iterative optimization in deep learning. \n",
    "\n",
    "- SGD, a variant of gradient descent algorithm, accelerates model learning in deep learning. \n",
    "\n",
    "- The term stochastic alludes to the random nature of the algorithm, SGD randomly picks one sample from a dataset to approximate the loss and the gradient and updates the parameters\n",
    "- optimal convergence is SGD can be achieved by systematically decreasing the learning rate over time, allowing the algorithm to settle closer to the global minimum. \n",
    "\n",
    "    - The global minimum is the point in the parameter space where the cost function attains its lowest possible value. \n",
    "    - At this point, the model parameters (weights and biases) are optimized so that the cost function is minimized globablly across the entire parameter space. \n",
    "\n",
    "    ## Key features of SGD\n",
    "    - optimization uses mini-batches of data to find optimized weights\n",
    "    - SGD shuffles the data points within each mini-batch for improved generalization.\n",
    "    - SGD aims to iteratively update the weights to find the optimal solution, considering ffactors like mini batch randomess and noise in the data. \n",
    "\n",
    "  ## Stochastic Gradient Descent mini batch (SGD mini batch)\n",
    "  - is a combination of vanilla GD and SGD, which distribute the training data in its entirety in mini batches\n",
    "  - it divides the training data into small batches so that the network can easily be trained on the data. \n",
    "  - The mathematical formulation is the same as vanilla GD, but the training occurs batch wise\n",
    "\n",
    "## Gradient Descent vs SGD mini batch\n",
    "- GD is computationally expensive, but it converges to the global minimum smoothly. In contrast, SGD creates more noisy weight, in turn, takes more time to reach the global minimum \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
