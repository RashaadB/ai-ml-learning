welcome to the Course 4, where we will be focusing on Deep learning. Learning about Deep Neural Networks (DNN), Artificial Neural Networks (ANN) and the perceptrons used for binary classification, TensorFlow used for building models, Model Optimization and performace improvements, Transfer learning and much more!


## Artificial Neural Network Folder

`intro_deep_learning.ipynb`
- intro to deep learning
- deep learning vs machine learning
- Applications of deep learning
- limitations of deep learning
- Keras, TensorFlow, PyTorch
- Deep Learing Lifecycle

`intro_artificial_neural_network.ipynb`
- Artificial Neuron
- Neural Networks and the different types
    - Perceptrons
    - Multilayer perceptron
    - Deep Neural Networks or DNNs 
    - Convolutional neural networks or CNNs
    - Recurrent neural networks or RNNs
- Feedforward Neural Networks
- Activation Function
- types of activation functions
- forward propagation in perceptrons
- backpropagation in perceptrons
- error landscapre
- vanishing gradient
- Exploding Gradient

`perceptron_classification.ipynb`
- Build perceptron-based classification model
- Perform preprocessing and splitting on data
- Initiakize and fit the perceptron

`neural_networks_activation_functions.ipynb`
- configuring Neural Network
- applying activation function
- applying the think function

## Deep Neural Networks Folder

`deep_neural_networks.ipynb`
- deep neural networks architecture
- Loss function in DNN and Types
    - regression loss
        - Mean Absolute error
    - classifon loss
        - Mean squareed error
    - Forward and Backward propagation in DNN

    `forward_propagation.ipynb`
    - predict data output
    - calculate the errors
    - calculate sum squared error
    

## TensorFlow Folder
`intro_tensors.ipynb`
- Tenors
- TensorFlow
- dataflow graph
- TensorFlow APIs
- TensorFlow Playground
- TFLearn
- Keras
     
`tensorFlow_practice.ipynb`
- TensorFlow hands on
- Create sequential model
- applying softmax activation
- Create probability model

`deep_neural_networks_tensorflow.ipynb`

<!-- Building Deep Neural Networks on TensorFlow refers to the process of designing and constructing neural network models using the TensorFlow framework. This involves defining the architecture of the neural network, selecting appropriate layers and activation functions, specifying the optimization algorithm, and training the model using data. -->

`sequential_apis_in_tensorflow.ipynb`
<!-- The sequential API in TensorFlow offers a high-level interface for building and training deep learning models. It allows for the sequential addition of layers, simplifying the process of constructing neural network architectures by specifying the input shape and layer type. -->
- load fashion data set
- build model
- train test split
- compile the model
- evaluate the model
- predict the model

`functional_apis_tensorflow.ipynb`
<!-- The functional APIs in TensorFlow is an alternative way to create and customize complex neural network models. It allows you to build models with more flexibility and handle multiple inputs and outputs. -->
- load a dataset 
- inspect and visualize dataset
- build neural network modle using functional API
- compile model
- evaluate the model

`tensorflow_practice_2`
- load dataset
- define classifiers
- voting classifier
- train and evaluate classifiers
- visualization of decision boundaries

## PyTorch 
`pytorch_intro.ipynb`
- Features of PyTorch
- Modules in PyTorch
    - Basic Layers
    - Activation Function
    - Pooling Layers
    - Normalization Layers
    - Dropout Layers

`torch_dl_model.ipynb`
<!-- build and train a deep learning model using the FashionMNIST dataset, which consists of 28x28 grayscale images of 10 different clothing items. The model, a convolutional neural network (CNN), is trained to classify these images into their respective categories, leveraging PyTorch for the implementation and employing techniques such as normalization and Adam optimization to enhance performance. -->

`torch_classifier_model.ipynb`
<!-- In this example, we develop and train a deep learning model utilizing the MNIST dataset, which comprises 28x28 grayscale images of handwritten digits from 0 to 9. The model, a fully connected neural network, is meticulously designed to classify these images into their respective digit categories. We employ PyTorch, a powerful and flexible deep learning framework, to facilitate our implementation. -->

## Model Optimization & Performance Improvement Folder
`model_opt_perfor_improvement.ipynb`
- Optimization Algorithms
- Importance of Optimization Algorithms
- Optimizers and their Types
    - Gradient Descent
    - Stochastic Gradient Descent
    - Stochastic Gradient Descent mini batch
    - Momentum
    - Nesterov Accelerated Gradient (NAG)
    - RMSProp
    - Adadelta
    - Adam Optimizer
- Batch Normalization
- Regularization
- Modifying the loss function
- loss function strategies
- Data augmentation
- K fold cross-validation
- Vanishing Gradient
- Prevent vanishing gradient
- Exploding Gradient
- Hyperparameter and Parameters






