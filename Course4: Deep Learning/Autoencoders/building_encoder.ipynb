{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOQhF2KMWIdp"
   },
   "source": [
    "# __Building and Visualizing an Autoencoder with the Fashion MNIST Dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnDyF-j2WIdw"
   },
   "source": [
    "Autoencoders are a special type of neural network used for:\n",
    "\n",
    "- Data compression\n",
    "- Feature extraction\n",
    "- Dimensionality reduction\n",
    "- Learning generative models of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuB6nHmVr7vm"
   },
   "source": [
    "## Steps to be followed\n",
    "1. Import the libraries\n",
    "2. Load the dataset and find the shape of the data\n",
    "3. Initialize the autoencoder\n",
    "4. Compile the autoencoder\n",
    "5. Train the model\n",
    "6. Visualize the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xndySBmPWIdx",
    "tags": []
   },
   "source": [
    "###Step 1: Import the libraries\n",
    "- Import the required libraries, such as NumPy, Pandas, TensorFlow, and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 15079,
     "status": "ok",
     "timestamp": 1719158533520,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "0KteOPBSWIdy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 07:16:17.233190: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 07:16:19.236901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-05 07:16:19.886265: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-05 07:16:20.068097: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 07:16:21.468258: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 07:16:25.175348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1719159817871,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "fsaIjTR76PFW"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCiO00Q0WId0",
    "tags": []
   },
   "source": [
    "### Step 2: Load the dataset and find the shape of the data\n",
    "\n",
    "- Dataset used: Fashion MNIST dataset, where each image is 28 *28 pixels\n",
    "- Find the shape of the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1719158554961,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "-o4rX184WId0",
    "outputId": "9498e9ab-ba2c-4601-aac8-87fee7b5fbdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVJiLZavYx-K"
   },
   "source": [
    "__Observation:__\n",
    "- Here, the shape function retrieves the number of rows and columns present in the train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdg2ePgTWId2"
   },
   "source": [
    "### Step 3: Initialize the autoencoder\n",
    "- Define the value of __latent_dim__ as __64__.\n",
    "- Define a class called __Autoencoder__ that extends the __Model__ class from TensorFlow.\n",
    "- Inside the Autoencoder class, define the constructor (__init__) that takes __latent_dim__ as a parameter.\n",
    "- In the constructor, set __self.latent_dim__ to the value of __latent_dim__.\n",
    "- Define the encoder part of the autoencoder using __tf.keras.Sequential__.\n",
    "- In the encoder, flatten the input using __layers.Flatten()__.\n",
    "- Add a dense layer to the encoder with latent_dim units and ReLU activation using __layers.Dense(latent_dim, activation='relu')__.\n",
    "- Define the decoder part of the autoencoder using __tf.keras.Sequential__.\n",
    "- In the decoder, add a dense layer with __784__ units and sigmoid activation using __layers.Dense(784, activation='sigmoid')__.\n",
    "- Reshape the output of the dense layer to a 28x28 shape using __layers.Reshape((28, 28))__.\n",
    "- Define the call method of the __Autoencoder__ class.\n",
    "- Inside the call method, pass the input x through the encoder to obtain the encoded representation.\n",
    "- Pass the encoded representation through the decoder to obtain the reconstructed output.\n",
    "- Return the reconstructed output.\n",
    "- Create an instance of the __Autoencoder__ class called autoencoder, passing the value of __latent_dim__ as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1719158684477,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "GnI5ol-dWId3"
   },
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(784, activation='sigmoid'),\n",
    "      layers.Reshape((28, 28))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AKrupUebYNP"
   },
   "source": [
    "__Observations:__\n",
    "- The code does not produce any output by itself. It defines a class and creates an instance of that class.\n",
    "- The output will depend on how the autoencoder model is trained and used further in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ODpG7WnbLkT"
   },
   "source": [
    "### Step 4: Compile the autoencoder\n",
    "- Call the __compile()__ method on the autoencoder object.\n",
    "- Set the optimizer argument to __adam__. This specifies that the Adam optimizer will be used for training the autoencoder.\n",
    "- Set the loss argument to __losses.MeanSquaredError()__. This specifies that the mean squared error loss function will be used for training the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1719158691080,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "FF7eAXGqWId4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 07:16:45.143826: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OitKLTwJbpXU"
   },
   "source": [
    "__Observation:__\n",
    "- It configures the autoencoder model for training by setting the optimizer and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y7Dla_hWId4"
   },
   "source": [
    "### Step 5: Train the model\n",
    "- Call the __fit()__ method on the autoencoder object.\n",
    "- Pass __x_train__ as the first argument. x_train represents the input data for training the autoencoder.\n",
    "- Pass __x_train__ again as the second argument. This is the target data for the autoencoder, which is also x_train in this case.\n",
    "- Set the __epochs__ argument to __10__. This specifies the number of times the entire dataset will be iterated during training.\n",
    "- Set the __shuffle__ argument to __True__. This indicates that the training data will be shuffled before each epoch during training.\n",
    "- Set the validation_data argument to __(x_test, x_test)__. This provides the validation data needed to evaluate the performance of the autoencoder during training. x_test is the input validation data, and x_test is also used as the target validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83598,
     "status": "ok",
     "timestamp": 1719158793001,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "8Oi27ENdWId4",
    "outputId": "fa25032b-3b39-4516-aa07-4a136a66e463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0403 - val_loss: 0.0135\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0092 - val_loss: 0.0093\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0091 - val_loss: 0.0092\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0090 - val_loss: 0.0091\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0089 - val_loss: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9d9eef34c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX4XyfU6cndd"
   },
   "source": [
    "__Observations:__\n",
    "- The __fit()__ method trains the autoencoder model on the provided data and produces output.\n",
    "- During training, it displays information such as the loss and metrics for each epoch, the progress bar, and validation metrics if validation data is provided.\n",
    "- The final output is the trained autoencoder model with updated weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQOll3M5dh4C"
   },
   "source": [
    "### Step 6: Encode and decode the images\n",
    "- Call the encoder method of the autoencoder object on __x_test__. This encodes the input x_test using the trained autoencoder's encoder part.\n",
    "- Call the __numpy()__ method on the encoded output to convert it into a NumPy array.\n",
    "- This is done to extract the actual values from the TensorFlow tensor.\n",
    "- Assign the encoded output to the variable __encoded_imgs__.\n",
    "- Call the decoder method of the autoencoder object on encoded_imgs. This decodes the encoded images using the trained autoencoder's decoder part.\n",
    "- Call the numpy() method on the decoded output to convert it into a NumPy array.\n",
    "- Assign the decoded output to the variable __decoded_imgs__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1719158823649,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "xuX2iJQDWId5"
   },
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSTwoyvgeb6d"
   },
   "source": [
    "### Step 7: Display the images\n",
    "\n",
    "- Set up the figure and subplot layout.\n",
    "- Iterate through a range of n (in this case, 10) for displaying original and reconstructed images.\n",
    "- Display the original image in the current subplot, along with the __original__ title and grayscale colormap.\n",
    "- Display the reconstructed image in the next subplot, along with the __reconstructed__ title and grayscale colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "executionInfo": {
     "elapsed": 1636,
     "status": "ok",
     "timestamp": 1719158835408,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "9NT0tzMPWId6",
    "outputId": "59dbb9a5-e6dc-4b63-ee05-6d272f0ce65e"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "\n",
    "  ax = plt.subplot(2, n, i + 1)\n",
    "  plt.imshow(x_test[i])\n",
    "  plt.title(\"original\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  plt.imshow(decoded_imgs[i])\n",
    "  plt.title(\"reconstructed\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_m-QAxCeu4E"
   },
   "source": [
    "__Observations:__\n",
    "- The code generates a figure that showcases original images alongside their corresponding reconstructed images, with the __original__ and __reconstructed__ titles.\n",
    "- The images are displayed in grayscale."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
