{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ae1082",
   "metadata": {},
   "source": [
    "## Unsupervised Deep Learning\n",
    "\n",
    "- is a subset of machine learning techniques where models are trained using data that has not been labeled,categorized, or classified. The models identify patterns, relationships, and structures within the input data. \n",
    "\n",
    "    - Example: It is difficult to group everyones photo when there is a huge collection of photos. \n",
    "        - Unsupervised learning is used to group the photos based on some strucutral patterns it finds in the photos. \n",
    "        - grouping can be done based on the semantic information it learns from the images. \n",
    "\n",
    "## Unsupervised leaning approaches\n",
    "\n",
    "- `Cluster`: The techniques covered include K-means clustering, Hierarchical clustering, Density based spatial clustering of applications with noise. \n",
    "\n",
    "- `Association`: The algorithms used to generate association rules include Apriori algorithm, Eclat algorithm \n",
    "\n",
    "- `Dimensionality reduction`: the dimensionality reduction methods include Principal component analysis, Singular value decomposition, Autoencoders\n",
    "\n",
    "## Autoencoders\n",
    "\n",
    "- Autoencoders are a type of neural network used in unsupervised learning. They compress and encode data, then reconstruct it from the reduced encoded form. \n",
    "\n",
    "## Components of autoencoders\n",
    "\n",
    "- `encoder`: The encoder maps the input data to a lower dimensional representation, referred to as the code. the encoder typically consists of one or more fully connected layers that gradually decrease in size, leading to the bottleneck layer. \n",
    "\n",
    "- `bottleneck layer`: The bottleneck layer contains the compressed representation of the input data (code). Its dimensionality is typically much lower than the dimensionality of the input data. This compression forces the autoencoder to capture the most important features and discard noise or less relevant information\n",
    "\n",
    "\n",
    "- `decoder`: The decoder maps th elower dimensional representation code back to the original input data. The decoder works like the encoder in reverse. It takes the encoded information and uses a similar structure to decode it, creating a final output. It consists of layers that progressively increase in size until until reaching the size of the original input. This structure allows the decoder to expand the compressed code back into the full input. \n",
    "\n",
    "## Hyperparameters to train an autoencoder\n",
    "\n",
    "- `Code Size`: The code size is determined by the number of nodes in the bottleneck layer, which directly influences the quality of data compression. A smaller code size may enhance compression but at the risk of losing significant information. \n",
    "\n",
    "- `Number of layers`: There can be several layers on both the encoder and decoder sides. These layers facilitate the process of feature extraction. \n",
    "\n",
    "- `Number of nodes per layer`: The number of nodes typically decreases in the encoder layers to achieve dimensionality reduction and increases in the decoder layers to reconstruct the input. \n",
    "\n",
    "- `Loss function`: The means squared error or corss entropy is used as the loss function. \n",
    "\n",
    "## Autoencoder Use Cases\n",
    "\n",
    "- `Data denoising`: Autoencoders can effectively remove noise from images, reconstructing them to etheir original, clear format. \n",
    "\n",
    "- `Dimensionality reduction`: Autoencoders can reduce the dimenstionality of data, preserving essential features while significantly compressing the data. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
