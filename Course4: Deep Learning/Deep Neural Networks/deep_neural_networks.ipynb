{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ee5d33",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "- refers to a type of artificial neural network that consists of hidden layers between the input and output layers.\n",
    "\n",
    "- Depth refers to number of hiddlen layers, which is the key factor that distinguishes deep neural networks from traditional neural networks, enabling DNNs to outperform NNs in various tasks. \n",
    "\n",
    "- DNNs offer higher accuracy and have the ability to emulate the dcision making process of the human brain more effectively\n",
    "    - Example: consider a DNN designed and trained to recognize dog breeds\n",
    "        - It canalyze an image of a daog and predict its specific breed based on probability calculations.\n",
    "        - The breed with the highest probability is usually chosen as the predicted breed. \n",
    "\n",
    "## Benefits of DNN\n",
    "- Online translation\n",
    "- Self-driving cars\n",
    "- voice assistants\n",
    "- image and voice recognition software\n",
    "- chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00fe7e",
   "metadata": {},
   "source": [
    "## Loss Function in DNN and Types\n",
    "- It is used to measure the discrepancy between predicted and actual values, enabling the network to learn and improve its performance during training. \n",
    "- The model being developed needs to be continually assessed for potential errors as part of the optimization process. \n",
    "\n",
    "    - example of loss function: \n",
    "\n",
    "        - Assign labels to two images: 0 to a dog and 1 to the cat\n",
    "        - classify all images of dogs and cats\n",
    "        - if the model receives an image of a dog and identifies the output as 0.25, the difference in the model's prediction and the label is: 0.25 - 0 = 0.25\n",
    "        - the difference is known as the error in the model.\n",
    "        - every output undergoes this precise process, and the error is gathered from each individual output during the learning iterations. \n",
    "\n",
    "## Loss function categories\n",
    "\n",
    "- loss functions are fundamental tools in evaluating deep learning models, and they can be categorized into Regression loss and classification loss\n",
    "\n",
    "- `Regression loss`: Neural networrk regression predicts continuous values using inputs features and a suitable loss function. the output the model is value that could be like salary of an employee\n",
    "\n",
    "    - Types of Regression Loss:\n",
    "\n",
    "        - ` Mean absolute error(MAE)`: Mean absolute error measures the average absolute difference between the predicted and actural values regression tasks. \n",
    "\n",
    "            - Mean absolute error is used as a metric to measure the average magnitude of errors between predicted and actual values in regression problems\n",
    "\n",
    "        - `Mean squared error (MSE)`: Mean squared Error measures the average squared difference between predicted and true values in regression tasks\n",
    "            - if multiple samples are passed at the same time, the mean of the squared errors over all the samles can be taken. MSE is a standard loss function that can be implemented.\n",
    "\n",
    "            - can be used when evaluating the mean squared error (MSE metric), which measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "            - MSE is commonly used in backpropagation due to its empirical effectiveness in minimizing squared difference between predicted and actual values\n",
    "\n",
    "    - Types of Classification Loss:\n",
    "\n",
    "        - ` Cross-entropy loss`: is a mathematical measure used to quantify the difference between probability distributions. If the predicted probability distribution is not close to the actual value, the model adjusts its weight\n",
    "\n",
    "        - `Hinge loss`: is a loss function utilized within machine learning to train classifiers. It is optimized to increase the margin between data points and the decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752282c",
   "metadata": {},
   "source": [
    "## Forward Propagation in Deep Neural Networks\n",
    "- Forward propagation involves feeding input data through a neural network to produce an output. The data passes through hidden layers, where each layer processes it using an activation function before passing it to the next layer. This forward flow ensures data does not circulate back, preventing non-output states.\n",
    "\n",
    "- The process occurs for each layer in the network until the input reaches the output layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b0182",
   "metadata": {},
   "source": [
    "## Backward Propagation in Deep Neural Network\n",
    "- It refers to the practice of adjesting the weights of a neural network based on the error rate or loss clollected. It is the essence of neural net training and helps ensure lower error rates.\n",
    "\n",
    "- Backpropagation can indeed overtrain or overfit the models\n",
    "\n",
    "## Epochs \n",
    "-  refers to the number of times the entire dataset is passed forward and backward through a neural network during training. \n",
    "- The learned model may fit the training data, including its outliers (noise) too well but fails to generalize test data. \n",
    "\n",
    "## Regularization \n",
    "- Regularization is a method applied in machine learning and deep learning to avoid overfitting and enhance a model's ability to generalize. This technique introduces a penalty into the loss function during the training process.\n",
    "\n",
    "## commonly used regularization techniques:\n",
    "\n",
    "- `L2 regularization`: weight decay, also known as L2 regularization, is a specific type of regularization taht adds the squared magnitude of the weights to the cost function. \n",
    "\n",
    "- Regularization penalizes large weights in addition to the overall cost function. \n",
    "\n",
    "- The weight decay value determines how dominant regularization is during gradient computation. \n",
    "\n",
    "- A large weight decay coefficient implies a large penalty for large weights\n",
    "\n",
    "- `Dropout Regularization`: Dropout is a technique used during training in neural networks to prevent overfitting by randomly dropping units (neurons) and their connections with a certain probability, p. \n",
    "\n",
    "- randomly drop units (along with their connections) during training. \n",
    "- each unit is retained with a fixed probability p, independent of other units.\n",
    "- The hyper-parameter p has to be chosen (tuned)\n",
    "- In dropout regularization, during training, a fraction of the weights are randomly set to zero. \n",
    "\n",
    "- the goal is to prevent overfitting and improve the generalization capability of the neural network by randomly deactivating units during training. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
