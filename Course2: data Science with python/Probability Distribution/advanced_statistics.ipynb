{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3881c2e",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and Mechanism\n",
    "# Introduction to Hypothesis\n",
    "- It is a statement that proposes a possible explanation for an observed phenomenon or relationship, which scientists can test through investigation.\n",
    "\n",
    "## Hypothesis Components\n",
    "- A hypothesis has two components:\n",
    "\n",
    "- Example:\n",
    "    - If you do not clean the fish tank once every three days, the fish will probably not survive for more than three months.\n",
    "\n",
    "    - Do not clean the fish tank once every three days is the independent variable.\n",
    "    - Fish will probably not survive for more than three months is the dependent variable.\n",
    "\n",
    "\n",
    "## Null and Alternative Hypothesis\n",
    "\n",
    "- There are two types of hypotheses:\n",
    "\n",
    "    - Null hypothesis [ ùêª0 ]\n",
    "    - Alternative hypothesis [ ùêªùëé ]\n",
    "\n",
    "- Example: \n",
    "- Sales Promotion Drive:\n",
    "\n",
    "- Hypothesis: A sales promotion drive will increase the average monthly sales [¬µ] by 500 units.\n",
    "\n",
    "- According to the hypothesis, the sales promotion drive will increase the average monthly sales (¬µ) by 500 units.\n",
    "\n",
    "- According to the null hypothesis ( ùêª0 ), the sales promotion drive has an insignificant impact on the average monthly sales. The historical average ( ¬µ0  ) holds ¬µ =  ¬µ0 .\n",
    "\n",
    "\n",
    "- According to the alternative hypothesis ( ùêªùëé ), the sales promotion drive has a significant impact on the average monthly sales. Thus, ¬µ > ¬µ0.\n",
    "\n",
    "- Hourly Output of Two Machines:\n",
    "\n",
    "    - The null hypothesis states that the average hourly output of machine A (¬µ1) does not significantly differ from that of machine B (¬µ2). So, ¬µ1 = ¬µ2.\n",
    "\n",
    "\n",
    "    - The alternative hypothesis is that the average hourly output of machine A (¬µ1) is significantly larger than that of machine B (¬µ2). So, ¬µ1 > ¬µ2.\n",
    "\n",
    "Generally, a null hypothesis is the negation of the assertion, while an alternative hypothesis in itself is an assertion.\n",
    "\n",
    "## Hypothesis Testing\n",
    "Hypothesis testing verifies a hypothesis's plausibility using sample data. A sample data may come from a larger population of data or even from data-generating experimentation.\n",
    "\n",
    "- A hypothesis test always involves four steps. These are:\n",
    "\n",
    "    1. State the hypotheses: A statement of the correctness of the two hypotheses (null or alternative)\n",
    "\n",
    "    2. Set the criteria for a decision: A plan that outlines how to evaluate data\n",
    "\n",
    "    3. Compute sample statistics: Execution of the plan to physically carry out the analysis\n",
    "\n",
    "    4. Make a decision: Analysis of the result that rejects the null hypothesis or states that the null hypothesis is plausible\n",
    "\n",
    "## Hypothesis Testing Outcomes: Type I and Type II Errors\n",
    "- There are four decisions and outcomes for hypothesis testing:\n",
    "\n",
    "\n",
    "    - ùêª0  (null hypothesis) is TRUE and it is rejected: this is a Type I error\n",
    "\n",
    "    - ùêª0  (null hypothesis) is TRUE and it is accepted: correct decision\n",
    "\n",
    "    - ùêª0  (null hypothesis) is FALSE and it is rejected: correct decision\n",
    "\n",
    "    - ùêª0  (null hypothesis) is FALSE and it is accepted: this is a Type II error\n",
    "\n",
    "    - The probability of the occurrence of Type I errors is denoted by Alpha (Œ±), and the probability of Type II errors is denoted by Beta (Œ≤).\n",
    "\n",
    "- It is indeed not possible to make both Œ± and Œ≤ zero at the same time when inferences are based on samples. However, reducing one typically increases the other, given a fixed sample size. They're usually not equal to one, and in fact, you often want both to be as small as possible.\n",
    "\n",
    "- Common choices for Œ± include 0.05 or 0.01. To achieve a low Œ≤ (and thus high power), researchers typically use large sample sizes, careful experimental design, and sometimes more sophisticated statistical techniques.\n",
    "\n",
    "- The selected value of Alpha is known as the level of significance. For example, when Alpha is equal to 0.05, the level of significance is 5%.\n",
    "\n",
    "- This is a simple and effective way to estimate the outcomes of hypothesis testing.\n",
    "\n",
    "## Steps Involved in Hypothesis Testing\n",
    "Create a null hypothesis and an alternative hypothesis.\n",
    "\n",
    "- Decide on a level of significance, that is, alpha = 5% or 1%.\n",
    "\n",
    "- Choose the type of test you want to perform per the sample data (z-test, t-test, or chi-square).\n",
    "\n",
    "- Calculate the test statistics (z-score, t-score) using the respective formula of the test chosen.\n",
    "\n",
    "- Obtain the critical value in the sampling distribution to construct the rejection region of size alpha using the z-table, t-table, or chi table.\n",
    "\n",
    "- Compare the test statistics with the critical value and locate the position of the calculated test statistics, that is, see if it is in the rejection region or non-rejection region.\n",
    "\n",
    "    - If the critical value lies in the rejection region, you will reject the hypothesis, that is, sample data provides sufficient evidence against the null hypothesis, and there is a significant difference between hypothesized value and the observed value of the parameter.\n",
    "\n",
    "    -  If the critical value lies in the non-rejection region, you will not reject the hypothesis, that is, the sample data does not provide sufficient evidence against the null hypothesis, and the difference between hypothesized value and the observed value of the parameter is due to the fluctuation of the sample.\n",
    "\n",
    "## Confidence Interval\n",
    "- A confidence interval (CI) generally indicates the amount of uncertainty‚ÄØin any distribution. It is usually expressed as a number or a set or pair of numbers, and it can even be computed for a given distribution statistic.\n",
    "\n",
    "- CI is the probability that a particular‚ÄØpopulation‚ÄØparameter will fall between a set of values for a certain period.\n",
    "\n",
    "- CIs can take any number of probability limits, the most common being 95%, and in some cases, even 99%. When the behavior of a population is not known, then it is required to deduce the confidence intervals based on the sample data using statistical methods like a‚ÄØT-Test.\n",
    "\n",
    "- A confidence interval‚ÄØis essentially a range of values that bind the statistic's‚ÄØmean value, which could in turn contain an unknown population parameter.\n",
    "\n",
    "- A typical confidence interval in a statistical distribution is shown below:\n",
    "\n",
    "\n",
    "- An upper limit and a lower limit of CI are marked on either side of the distribution.\n",
    "\n",
    "## Margin of Error\n",
    "The margin of error (MoE) indicates by how many‚ÄØpercentage points‚ÄØthe results will differ‚ÄØfrom the real population value.\n",
    "\n",
    "Consider the following statement:\n",
    "\n",
    "A 95%‚ÄØconfidence level‚ÄØwith a 3% margin of error implies that the‚ÄØstatistical distribution data‚ÄØis within 3% points of the real population value 95% of the time.\n",
    "\n",
    "The MoE is thus an important part of the confidence interval, without which one can‚Äôt accept the inference from statistical analysis. The lower the margin of error, the better the acceptability of the population statistic. MoE is popularly used in poll and election surveys. A poll survey MoE must be scrutinized before accepting the confidence interval.\n",
    "\n",
    "For Example:\n",
    "\n",
    "Consider the Gallup poll survey conducted in the 2012 US Presidential elections. The survey indicated 49% voting in favor of Mitt Romney, and 47% in favor of Barack Obama, with 95% CI and +/- 2% MoE. However, Barack Obama polled 51%, while Mitt Romney got 47% in the actual election. The results were even outside the range of the Gallup poll‚Äôs MoE of +/-2%. This illustrates the need for statistics while taking CI, CL, and MoE into consideration.\n",
    "\n",
    "## Confidence Levels\n",
    "A confidence level is the percentage of probability or certainty that the confidence interval will contain the true population parameter when a random sample is drawn repeatedly.\n",
    "\n",
    "- In statistics, confidence levels are expressed as a‚ÄØpercentage‚ÄØ(for example, a 99%, 95%, or 80% confidence level). However, to support or discard the null hypothesis, scientists and engineers usually work with a level of 95% or more. On the other hand, most governmental organizations and departments use 90% as the limit for confidence level.\n",
    "\n",
    "- A graphical representation of a confidence level of 95% is shown below:\n",
    "\n",
    "- A confidence level of 95% means that when the experiment is repeated or the poll survey is conducted repeatedly, the survey results will match the results from a population 95% of the time. Confidence levels for different fields are different and are usually adopted in consultation with domain experts. This is done to ensure that the prediction from the statistic is reliable.\n",
    "\n",
    "## Z-Distribution (Standard Normal Distribution)\n",
    "\n",
    "- Shape: The Z-distribution has a bell-shaped curve, symmetric around the mean.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "- The mean, median, and mode of the distribution are all zero.\n",
    "It has a standard deviation of one.\n",
    "The total area under the curve is equal to 1.\n",
    "Usage:\n",
    "\n",
    "- It is used for hypothesis testing and confidence intervals when the sample size is large and the population standard deviation is known.\n",
    "Any normal distribution can be standardized by converting values into z score.\n",
    "\n",
    "## T-Distribution\n",
    "\n",
    "- Shape: The T-distribution is similar to the Z-distribution, but it has heavier tails. This means it is more prone to producing values that fall far from its mean.\n",
    "\n",
    "- Degree of Freedom: The T-distribution uses only one parameter, which is called the degree of freedom (df). It refers to the number of independent observations, that is, if n = total no. of observations, then df = n-1\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "- It's symmetric and bell-shaped, like the Z-distribution, but its exact shape depends on the degrees of freedom.\n",
    "- The mean of the T-distribution is zero, and its variance is slightly greater than 1 (This varies depending on the degrees of freedom.).\n",
    "- As the degrees of freedom increase, the T-distribution approaches the Z-distribution.\n",
    "Usage:\n",
    "\n",
    "- The T-distribution is used when the sample size is small and the population standard deviation is unknown. It's particularly useful for estimating the mean of a normally distributed population in situations where the sample size is small.\n",
    "\n",
    "## Plotting T- and Z-Distribution\n",
    "- The plot below illustrates the differences between the Z-distribution (standard normal distribution) and the T-distributions with different degrees of freedom:\n",
    "\n",
    "\n",
    "- Z-Distribution: Represented by the solid line, this standard normal distribution is symmetric and bell-shaped, centering around zero with a standard deviation of one.\n",
    "\n",
    "- T-Distribution (df=5): The dashed line represents the T-distribution with 5 degrees of freedom. It's also symmetric and bell-shaped but has thicker tails compared to the Z-distribution. This indicates a higher likelihood of values far from the mean.\n",
    "\n",
    "- T-Distribution (df=30): The dotted line represents the T-distribution with 30 degrees of freedom. While it still has slightly thicker tails than the Z-distribution, it's much closer to the \n",
    "\n",
    "- Z-distribution's shape than the T-distribution, which has fewer degrees of freedom.\n",
    "\n",
    "## T-Test\n",
    "- A T-test is a statistical hypothesis-testing tool that is primarily utilized when the population variance is unknown and the sample size is relatively small (n < 30). There are two main types of T-tests: one-sample and two-sample T-tests.\n",
    "\n",
    "For a one-sample T-test\n",
    "\n",
    "- The one-sample T-test is used to compare the mean of a sample with a known population mean. This test employs the standard deviation of the sample instead of the population standard deviation due to the unknown population variance. The formula for a one-sample T-test is as follows:\n",
    "\n",
    "\n",
    "- Where xÃÑ is the sample mean, s is the standard deviation of the sample, Œº is the mean of the population, and n is the sample size.\n",
    "\n",
    "- For a two-sample T-test:\n",
    "\n",
    "- The two-sample T-test, on the other hand, is conducted to compare the means of two different samples to determine whether the differences between these two means are statistically significant. The formula for a two-sample T-test is:\n",
    "\n",
    "\n",
    "- For a paired sample t-test:\n",
    "\n",
    "- The paired sample T-test, also known as the dependent sample T-test, is used to compare the means of two related groups to determine if there is a statistically significant difference between these means. It is typically used when the same subjects are used for each treatment (for example, before and after a treatment in a medical study). The assumption is that the paired differences are approximately normally distributed. The formula for a paired sample T-test is:\n",
    "\n",
    "\n",
    "Where,\n",
    "\n",
    "- ùê∑¬Ø  is the mean of the differences between the paired observations.\n",
    "\n",
    "- ùë†ùê∑  is the standard deviation of these differences.\n",
    "\n",
    "- n is the number of pairs.\n",
    "\n",
    "- The paired sample T-test is particularly useful for before and after studies.This test accounts for the fact that the two groups are related and not independent of each other.\n",
    "\n",
    "##  Z-Test\n",
    "It is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large (n>30). There are two main types of Z-tests: one-sample and two-sample Z-tests.\n",
    "\n",
    "- For a one-sample Z-test\n",
    "\n",
    "- The one-sample Z-test is used to compare a population mean with the sample mean. The formula for a one-sample Z-test is as follows: link text\n",
    "- Where xÃÑ is the sample mean, œÉ is the standard deviation of the population, Œº is the mean of the population, and n is the sample size.\n",
    "\n",
    "- For a two-sample Z-test\n",
    "\n",
    "- The two-sample Z-test is used to compare the means of two different samples. The formula for a two-sample Z-test is as follows:\n",
    "## Choosing between T-Test and Z-Test\n",
    "\n",
    "- Sample Size\n",
    "    - Z-test: It is used when the sample size is large (usually, n ‚â• 30). The larger the sample size, the more the sample mean's distribution will resemble a normal distribution due to the central limit theorem.\n",
    "    - T-test: It is preferred for smaller sample sizes (n < 30). The T-test is more adaptable to small sample sizes since it accounts for the extra uncertainty introduced by estimating the population standard deviation.\n",
    "\n",
    "-  Population Standard Deviation\n",
    "    - Z-test: It is required that the population standard deviation is known. This circumstance is less common in real-world scenarios because having access to the entire population data typically implies having the population standard deviation.\n",
    "    - T-test: It is used when the population standard deviation is unknown and is estimated using the sample standard deviation. The T-test adjusts for the fact that the sample standard deviation varies between samples.\n",
    "\n",
    "- Distribution of the Data\n",
    "    - Z-test: It assumes that the data follows a normal distribution. This assumption becomes less of a concern with large sample sizes due to the central limit theorem.\n",
    "    - T-test: It is more suitable to use non-parametric tests when you are unsure if the data is normally distributed, particularly with smaller sample sizes.\n",
    "\n",
    "## P-Value\n",
    "- The p-value is a crucial component in statistical hypothesis testing. It is the smallest level of significance at which you can reject a null hypothesis. Since the p-value offers more information than the critical value, it is generally recommended in many statistical tests.\n",
    "- The definition and interpretation of the p-value are essentially measures of the probability that the observed data would be at least as extreme as the current results, given that the null hypothesis is true.\n",
    "- The application and interpretation of the p-value depend on the nature of the hypothesis test being conducted. Here's how it applies to different types of tests:\n",
    "\n",
    "- Right-Tailed Test:\n",
    "    - In a right-tailed test, the right end (larger values) of the distribution. The p-value in this case is calculated as:\n",
    "    - P-Value = P[Test statistics >= observed value of the test statistic]\n",
    "    - Left-Tailed Test:\n",
    "    - In a left-tailed test, the left end (smaller values) of the distribution. The p-value is then calculated as:\n",
    "    - P-Value = P[Test statistics <= observed value of the test statistic]\n",
    "\n",
    "- Two-Tailed Test:\n",
    "\n",
    "    - A two-tailed test is used for differences in either direction (larger or smaller). The p-value in a two-tailed test is calculated as:\n",
    "    - P-Value = 2 * P[Test statistics >= observed value of the test statistic]\n",
    "    * Remember that a smaller p-value provides stronger evidence against the null hypothesis.\n",
    "\n",
    "## Decision-Making Using P-Value\n",
    "- The p-value is compared to the significance level (alpha) for decision-making on the null hypothesis.\n",
    "- If the p-value is greater than alpha, you do not reject the null hypothesis.\n",
    "- If the p-value is smaller than the alpha, you reject the null hypothesis.\n",
    "\n",
    "    - One-Sample T-Test: Example\n",
    "    - For a particular organization, the average age of the employees was claimed to be 30 years. The authorities collected a random sample of 10 employees' age data to check the claim made by the organization. Construct a hypothesis test to validate the hypothesis at a significance level of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee52e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and dataset\n",
    "from scipy.stats import ttest_1samp\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "ages = pd.read_csv(\"Ages.csv\")\n",
    "\n",
    "# Assuming 'age' is the column of interest\n",
    "age_column = ages['ages']  # Replace 'age' with the actual column name\n",
    "mean_age = age_column.mean()\n",
    "print(f\"Mean age: {mean_age}\")\n",
    "\n",
    "# Perform one-sample t-test\n",
    "t_statistic, p_value = ttest_1samp(age_column, 30)\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Decision-making\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e20aba",
   "metadata": {},
   "source": [
    "## Paried Sample T-test\n",
    "\n",
    "- For a particular hospital, it is advertised that a particular chemotherapy session does not affect the patient's health based on blood pressure. It is to be checked if the blood pressure before the treatment is equivalent to the blood pressure after the treatment. Perform a statistical test at the aplha 0.05 level to help validate the claim.\n",
    "\n",
    "- $H_0$ = mean difference between two samples is 0\n",
    "- $H_1$ = mean difference between two samples is not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "df = pd.read_csv(\"blood_pressure.csv\")\n",
    "df.head()\n",
    "\n",
    "df[['bp_before','bp_after']].describe()\n",
    "ttest,pval = stats.ttest_rel(df['bp_before'], df['bp_after'])\n",
    "print(pval)\n",
    "if pval<0.05:\n",
    "    print(\"reject null hypothesis\")\n",
    "else:\n",
    "    print(\"accept null hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c8382",
   "metadata": {},
   "source": [
    "## Two-Sample T-Test: Example\n",
    "- Employee satisfaction is a crucial factor that can influence the productivity and success of a company. The Human Resources department wants to assess whether the satisfaction levels are consistent across different departments. For this analysis, we will focus on two key departments: Sales and Marketing\n",
    "\n",
    "- Objective: To determine if there is a statistically significant difference in the average employee satisfaction scores between the sales and marketing departments\n",
    "\n",
    "- Null Hypothesis ( ùêª0 ): There is no significant difference in the average satisfaction scores between employees in the Sales department and those in the Marketing department. (Mean_Sales = Mean_Marketing)\n",
    "\n",
    "- Alternative Hypothesis ( ùêª1 ): There is a significant difference in the average satisfaction scores between employees in the Sales department and those in the Marketing department. (Mean_Sales ‚â† Mean_Marketing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57771b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('employee_satisfaction.csv')\n",
    "\n",
    "# Separate the satisfaction scores for each department\n",
    "sales_scores = data[data['Department'] == 'Sales']['Satisfaction_Score']\n",
    "marketing_scores = data[data['Department'] == 'Marketing']['Satisfaction_Score']\n",
    "\n",
    "# Perform the independent two-sample t-test\n",
    "t_stat, p_val = stats.ttest_ind(sales_scores, marketing_scores, equal_var=False)\n",
    "print(f\"P-value: {p_val}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_val < alpha:\n",
    "    print(\"Reject the null hypothesis - there is a significant difference in satisfaction scores between departments\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - no significant difference in satisfaction scores between departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fe5b2",
   "metadata": {},
   "source": [
    "## Z-test\n",
    "- A school principal claims that the students in their school are more intelligent than those of other schools. A random sample of 50 students' IQ scores has a mean score of 110. The mean population IQ is 100, with a standard deviation of 15. State whether the claim of the principal is right or not at a 5% significance level.\n",
    "\n",
    "- ùêª0  = average population IQ score is 100\n",
    "\n",
    "- ùêªùëé  = average population IQ score is above 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839671b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# Generate a random array of 50 numbers having mean 110 and standard deviation of 15\n",
    "# similar to the IQ scores data\n",
    "mean_iq = 110\n",
    "sd_iq = 15/math.sqrt(50)\n",
    "alpha =0.05\n",
    "null_mean =100\n",
    "data = sd_iq*randn(50)+mean_iq\n",
    "# Print mean and SD\n",
    "print('mean=%.2f stdv=%.2f' % (np.mean(data), np.std(data)))\n",
    "\n",
    "# Now you perform the test, and in this function, you passed data in the value parameter\n",
    "# You passed mean value in the null hypothesis and will check if the mean is larger in the\n",
    "# alternative hypothesis\n",
    "\n",
    "ztest_Score, p_value= ztest(data,value = null_mean, alternative='larger')\n",
    "# The function outputs a p_value and z-score corresponding to that value, you compare the\n",
    "# p-value with alpha, and if it is greater than alpha, then you do accept the null hypothesis else you reject it.\n",
    "\n",
    "alpha = 0.05\n",
    "if(p_value < alpha):\n",
    "    print(\"Reject null Hypothesis\")\n",
    "else:\n",
    "    print(\"Fail to Reject null Hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa687b0",
   "metadata": {},
   "source": [
    "## Chi-Square Distribution\n",
    "- A chi-square distribution, pronounced khai square, is a continuous probability distribution widely used in statistical inference. The Greek letter œá is often used, and œá2 is termed chi-square.\n",
    "- The œá2 distribution and the standard normal distribution are related. If a random variable Z has a standard normal distribution, then Z2 has the œá2 distribution with one degree of freedom.\n",
    "\n",
    "    - With increasing degrees of freedom, the shape of the œá2 distribution varies. For k = 1, the PDF is infinity, when œá2 = 0. For k = 2, the PDF is 0.5 for œá2 = 0. For higher values of k (3 or more), the œá2 distribution changes to a positively skewed standard normal distribution, and with higher degrees of freedom, the skewness and the kurtosis of the œá2 distribution change, with the distribution becoming increasingly symmetric.\n",
    "\n",
    "    * In any œá2 distribution, the mean (Œº) is k, the number of degrees of freedom, and the variance is 2k.\n",
    "\n",
    "    * For example, for k = 3 in the diagram, Œº = 3, while the variance is 2 x k, or 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fe742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chi-square distribution of varying degrees of freedom\n",
    "data1 = np.random.chisquare(df = 1,size = 1000)\n",
    "data2 = np.random.chisquare(df = 2,size = 1000)\n",
    "data3 = np.random.chisquare(df = 3,size = 1000)\n",
    "print(data1[:10])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set the style of seaborn\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Plot the distributions using kdeplot\n",
    "sns.kdeplot(data1, label='dof 1')\n",
    "sns.kdeplot(data2, label='dof 2')\n",
    "sns.kdeplot(data3, label='dof 3')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "data = [[10,20,30],[6,9,17]]\n",
    "stat, p_value, dof, chi_array = chi2_contingency(data)\n",
    "p_value\n",
    "\n",
    "data = [[10,20,30],[9,1,8]]\n",
    "stat, p_value, dof, chi_array = chi2_contingency(data)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61c5e0",
   "metadata": {},
   "source": [
    "## Chi-Square Test and Independence Test\n",
    "- Statistical methods are often employed to understand the patterns and relationships in data. One such method is the chi-square test, which is used in two different but related scenarios: as a goodness-of-fit test and as a test for independence.\n",
    "\n",
    "- Chi-Square Test as a Test for Independence: A chi-square test for independence is used to determine whether there's a significant association between two categorical variables in a sample. It's non-parametric, meaning it does not assume any specific distribution for the variables involved.\n",
    "\n",
    "    - Define the Null and Alternative hypotheses based on the data.  ùêª0  implies that the data met the expected distribution, while  ùêª1  implies that it did not.\n",
    "\n",
    "    - State the alpha value as mentioned earlier; you usually work with a value of 0.05.\n",
    "\n",
    "    - Calculate the degrees of freedom, k. It depends on the number of categories or groups and is usually K-1, where K is the number of frequencies.\n",
    "\n",
    "    - State the decision rule. Calculate the decision value based on the alpha value and degrees of freedom. Based on this value, either reject  ùêª0  or reject  ùêª1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: \n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "# Load the chi-test.csv file\n",
    "df_chi = pd.read_csv('chi-test.csv')\n",
    "contingency_table=pd.crosstab(df_chi[\"Gender\"],df_chi[\"Shopping\"])\n",
    "print('contingency_table :-\\n',contingency_table)\n",
    "# Observed Values\n",
    "Observed_Values = contingency_table.values\n",
    "print(\"Observed Values :-\\n\",Observed_Values)\n",
    "b=stats.chi2_contingency(contingency_table)\n",
    "Expected_Values = b[3]\n",
    "print(\"Expected Values :-\\n\",Expected_Values)\n",
    "no_of_rows=len(contingency_table.iloc[0:2,0])\n",
    "no_of_columns=len(contingency_table.iloc[0,0:2])\n",
    "ddof=(no_of_rows-1)*(no_of_columns-1)\n",
    "print(\"Degree of Freedom:-\",ddof)\n",
    "alpha = 0.05\n",
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "critical_value=chi2.ppf(q=1-alpha,df=ddof)\n",
    "print('critical_value:',critical_value)\n",
    "# P-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('chi-square statistic:',chi_square_statistic)\n",
    "print('critical_value:',critical_value)\n",
    "print('p-value:',p_value)\n",
    "if chi_square_statistic>=critical_value:\n",
    "    print(\"Reject $H_0$,There is a relationship between 2 categorical variables\")\n",
    "else:\n",
    "    print(\"Retain $H_0$,There is no relationship between 2 categorical variables\")\n",
    "\n",
    "if p_value<=alpha:\n",
    "    print(\"Reject $H_0$,There is a relationship between 2 categorical variables\")\n",
    "else:\n",
    "    print(\"Retain $H_0$,There is no relationship between 2 categorical variables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
