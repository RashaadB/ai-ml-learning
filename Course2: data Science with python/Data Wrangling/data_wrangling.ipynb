{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a099514c",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "- Data wrangling, also known as data munging or data preprocessing, is the process of cleaning, structuring, and transforming raw data into a format suitable for analysis.\n",
    "- It is a crucial step in the data preparation pipeline, aiming to make the data more accessible, understandable, and ready for various analytical tasks.\n",
    "- It involves dealing with missing values, handling outliers, transforming variables, and merging datasets, among other tasks.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "- Data collection is the process of gathering information from diverse sources to build a comprehensive dataset for analysis.\n",
    "- Sources may include databases, APIs (application programming interfaces), spreadsheets, or external files. Effective data collection ensures the availability of relevant and reliable information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07eaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by Loading the data into a Pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"HousePrices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a2d76",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "- After loading the data set, its now time to explore the dataset to gain insights into the structure and quality\n",
    "- This step involves using functions like df.head(), df.info(), and df.describe() to gain insights into the dataset's structure, data types, and statistical summaries. Checking for missing values, outliers, and inconsistencies is crucial to identify potential issues that need addressing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417862ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "#Displaying the last few rows of the DataFrame\n",
    "print(df.tail())\n",
    "\n",
    "# Providing information about the DataFrame, including data types and non-null counts\n",
    "print(df.info())\n",
    "\n",
    "# Displaying descriptive statistics of the DataFrame, such as mean, std, min, max, and so on.\n",
    "print(df.describe())\n",
    "\n",
    "# Displaying datatypes of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2ef71",
   "metadata": {},
   "source": [
    "## Accessing Rows using .iloc and .loc\n",
    "\n",
    "- Inspecting the dataset involves exploring its content\n",
    "- Using .iloc and .loc allows accessing specific rows based on integer-location or label-based indexing, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85b2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date              2014-05-02 00:00:00\n",
      "price                        313000.0\n",
      "bedrooms                          3.0\n",
      "bathrooms                         1.5\n",
      "sqft_living                      1340\n",
      "sqft_lot                         7912\n",
      "floors                            1.5\n",
      "waterfront                          0\n",
      "view                                0\n",
      "condition                           3\n",
      "sqft_above                       1340\n",
      "sqft_basement                       0\n",
      "yr_built                         1955\n",
      "yr_renovated                     2005\n",
      "street           18810 Densmore Ave N\n",
      "city                        Shoreline\n",
      "statezip                     WA 98133\n",
      "country                           USA\n",
      "Name: 0, dtype: object\n",
      "______________________________________________\n",
      "date             2014-05-02 00:00:00\n",
      "price                       463000.0\n",
      "bedrooms                         3.0\n",
      "bathrooms                       1.75\n",
      "sqft_living                     1710\n",
      "sqft_lot                        7320\n",
      "floors                           1.0\n",
      "waterfront                         0\n",
      "view                               0\n",
      "condition                          3\n",
      "sqft_above                      1710\n",
      "sqft_basement                      0\n",
      "yr_built                        1948\n",
      "yr_renovated                    1994\n",
      "street            Burke-Gilman Trail\n",
      "city                Lake Forest Park\n",
      "statezip                    WA 98155\n",
      "country                          USA\n",
      "Name: 10, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Access the first row using iloc\n",
    "result_iloc_0 = df.iloc[0]\n",
    "\n",
    "#Display the results for df.iloc[0]\n",
    "print(result_iloc_0)\n",
    "print(\"______________________________________________\")\n",
    "\n",
    "# Access the eleventh row using iloc\n",
    "result_iloc_10 = df.iloc[10]\n",
    "print(result_iloc_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56612853",
   "metadata": {},
   "source": [
    "## Checking for any Missing Values\n",
    "- missing data or null values can play a huge part in data analysis\n",
    "- missing values can skew, alter data outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d398918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date             0\n",
      "price            0\n",
      "bedrooms         0\n",
      "bathrooms        0\n",
      "sqft_living      0\n",
      "sqft_lot         0\n",
      "floors           0\n",
      "waterfront       0\n",
      "view             0\n",
      "condition        0\n",
      "sqft_above       0\n",
      "sqft_basement    0\n",
      "yr_built         0\n",
      "yr_renovated     0\n",
      "street           0\n",
      "city             0\n",
      "statezip         0\n",
      "country          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for any missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa6e28",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "- Handling missing data is crucial for maintaining data integrity. \n",
    "- Different ways include imputation (replacing missing values with estimated values), the removal of records with missing values, or using default values when appropriate.\n",
    "- to handling missing values in numerical columns of the dataset, utilizie iloc to select them, excluding text columns.\n",
    "- Focusing soley on columns that do not contain text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f22600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values using imputation of the dataset\n",
    "\n",
    "#filling in missing values from columns 1-13\n",
    "df_filled = df.fillna(df.iloc[:, 1:14].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3e828",
   "metadata": {},
   "source": [
    "## Dealing with Duplicates\n",
    "- Duplicates in a dataset can introduce bias and errors.\n",
    "- Identifying and handling duplicate recourds is essential for ensuring accurate analysis and reporting\n",
    "- By default, drop_duplicates() retains the first occurrence of a duplicate and removes subsequent ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d153461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Duplicate records\n",
    "df_no_duplicates = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388217e",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "- Data Cleaning involves correcting typographical errors, standardizing date formats, and resolving inconsistencies in categorical data labeling. \n",
    "- Standardizing data formats and units, ensures consistency and facilitates analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4aa10c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date         price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "0    2014-05-02  3.130000e+05       3.0       1.50         1340      7912   \n",
      "1    2014-05-02  2.384000e+06       5.0       2.50         3650      9050   \n",
      "2    2014-05-02  3.420000e+05       3.0       2.00         1930     11947   \n",
      "3    2014-05-02  4.200000e+05       3.0       2.25         2000      8030   \n",
      "4    2014-05-02  5.500000e+05       4.0       2.50         1940     10500   \n",
      "...         ...           ...       ...        ...          ...       ...   \n",
      "4595 2014-07-09  3.081667e+05       3.0       1.75         1510      6360   \n",
      "4596 2014-07-09  5.343333e+05       3.0       2.50         1460      7573   \n",
      "4597 2014-07-09  4.169042e+05       3.0       2.50         3010      7014   \n",
      "4598 2014-07-10  2.034000e+05       4.0       2.00         2090      6630   \n",
      "4599 2014-07-10  2.206000e+05       3.0       2.50         1490      8102   \n",
      "\n",
      "      floors  waterfront  view  condition  sqft_above  sqft_basement  \\\n",
      "0        1.5           0     0          3        1340              0   \n",
      "1        2.0           0     4          5        3370            280   \n",
      "2        1.0           0     0          4        1930              0   \n",
      "3        1.0           0     0          4        1000           1000   \n",
      "4        1.0           0     0          4        1140            800   \n",
      "...      ...         ...   ...        ...         ...            ...   \n",
      "4595     1.0           0     0          4        1510              0   \n",
      "4596     2.0           0     0          3        1460              0   \n",
      "4597     2.0           0     0          3        3010              0   \n",
      "4598     1.0           0     0          3        1070           1020   \n",
      "4599     2.0           0     0          4        1490              0   \n",
      "\n",
      "      yr_built  yr_renovated                    street       city  statezip  \\\n",
      "0         1955          2005      18810 Densmore Ave N  Shoreline  WA 98133   \n",
      "1         1921             0           709 W Blaine St    Seattle  WA 98119   \n",
      "2         1966             0  26206-26214 143rd Ave SE       Kent  WA 98042   \n",
      "3         1963             0           857 170th Pl NE   Bellevue  WA 98008   \n",
      "4         1976          1992         9105 170th Ave NE    Redmond  WA 98052   \n",
      "...        ...           ...                       ...        ...       ...   \n",
      "4595      1954          1979            501 N 143rd St    Seattle  WA 98133   \n",
      "4596      1983          2009          14855 SE 10th Pl   Bellevue  WA 98007   \n",
      "4597      2009             0          759 Ilwaco Pl NE     Renton  WA 98059   \n",
      "4598      1974             0         5148 S Creston St    Seattle  WA 98178   \n",
      "4599      1990             0         18717 SE 258th St  Covington  WA 98042   \n",
      "\n",
      "     country  \n",
      "0        USA  \n",
      "1        USA  \n",
      "2        USA  \n",
      "3        USA  \n",
      "4        USA  \n",
      "...      ...  \n",
      "4595     USA  \n",
      "4596     USA  \n",
      "4597     USA  \n",
      "4598     USA  \n",
      "4599     USA  \n",
      "\n",
      "[4600 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "#Cleaning data by standardizing formats\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#Displaying the dataframe after cleaning\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41cedf",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "-  Data transformation inclues converting data types, creating new features through feature engineering, and normalizing or scaling numeric values as needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627d8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date         price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "0    2014-05-02  3.130000e+05       3.0       1.50         1340      7912   \n",
      "1    2014-05-02  2.384000e+06       5.0       2.50         3650      9050   \n",
      "2    2014-05-02  3.420000e+05       3.0       2.00         1930     11947   \n",
      "3    2014-05-02  4.200000e+05       3.0       2.25         2000      8030   \n",
      "4    2014-05-02  5.500000e+05       4.0       2.50         1940     10500   \n",
      "...         ...           ...       ...        ...          ...       ...   \n",
      "4595 2014-07-09  3.081667e+05       3.0       1.75         1510      6360   \n",
      "4596 2014-07-09  5.343333e+05       3.0       2.50         1460      7573   \n",
      "4597 2014-07-09  4.169042e+05       3.0       2.50         3010      7014   \n",
      "4598 2014-07-10  2.034000e+05       4.0       2.00         2090      6630   \n",
      "4599 2014-07-10  2.206000e+05       3.0       2.50         1490      8102   \n",
      "\n",
      "      floors  waterfront  view  condition  sqft_above  sqft_basement  \\\n",
      "0        1.5           0     0          3        1340              0   \n",
      "1        2.0           0     4          5        3370            280   \n",
      "2        1.0           0     0          4        1930              0   \n",
      "3        1.0           0     0          4        1000           1000   \n",
      "4        1.0           0     0          4        1140            800   \n",
      "...      ...         ...   ...        ...         ...            ...   \n",
      "4595     1.0           0     0          4        1510              0   \n",
      "4596     2.0           0     0          3        1460              0   \n",
      "4597     2.0           0     0          3        3010              0   \n",
      "4598     1.0           0     0          3        1070           1020   \n",
      "4599     2.0           0     0          4        1490              0   \n",
      "\n",
      "      yr_built  yr_renovated                    street       city  statezip  \\\n",
      "0         1955          2005      18810 Densmore Ave N  Shoreline  WA 98133   \n",
      "1         1921             0           709 W Blaine St    Seattle  WA 98119   \n",
      "2         1966             0  26206-26214 143rd Ave SE       Kent  WA 98042   \n",
      "3         1963             0           857 170th Pl NE   Bellevue  WA 98008   \n",
      "4         1976          1992         9105 170th Ave NE    Redmond  WA 98052   \n",
      "...        ...           ...                       ...        ...       ...   \n",
      "4595      1954          1979            501 N 143rd St    Seattle  WA 98133   \n",
      "4596      1983          2009          14855 SE 10th Pl   Bellevue  WA 98007   \n",
      "4597      2009             0          759 Ilwaco Pl NE     Renton  WA 98059   \n",
      "4598      1974             0         5148 S Creston St    Seattle  WA 98178   \n",
      "4599      1990             0         18717 SE 258th St  Covington  WA 98042   \n",
      "\n",
      "     country  log_price  normalized_price  \n",
      "0        USA  12.653958          0.011771  \n",
      "1        USA  14.684290          0.089658  \n",
      "2        USA  12.742566          0.012862  \n",
      "3        USA  12.948010          0.015795  \n",
      "4        USA  13.217674          0.020684  \n",
      "...      ...        ...               ...  \n",
      "4595     USA  12.638396          0.011590  \n",
      "4596     USA  13.188775          0.020095  \n",
      "4597     USA  12.940612          0.015679  \n",
      "4598     USA  12.222930          0.007649  \n",
      "4599     USA  12.304106          0.008296  \n",
      "\n",
      "[4600 rows x 20 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/7216hsv562lbddcy7qb278zc0000gp/T/ipykernel_1613/2695413612.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df['log_price'] = df['price'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "# Creating a new feature and normalizing normalizing numeric values\n",
    "#check if 'price' column exisits in the Dataframe\n",
    "\n",
    "import numpy as np\n",
    "if 'price' in df.columns:\n",
    "    #use the natural logarith to create a new feature 'log_price'\n",
    "    df['log_price'] = df['price'].apply(lambda x: np.log(x))\n",
    "\n",
    "    # Normalize 'price' column and create a new feature 'normalized_price'\n",
    "    df['normalized_price'] = (df['price'] - df['price'].min()) / (df['price']).max() - (df['price'].min())\n",
    "\n",
    "    #Displaying the Dataframe with the new features\n",
    "    print(df)\n",
    "else:\n",
    "    print(' the column price does not exist in this dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db54fb",
   "metadata": {},
   "source": [
    "## Data Binning\n",
    "-  Data binning, known as discretization, is a technique in data transformation to convert continuous numerical data into discrete bins or intervals.\n",
    "-  This process helps simplify the analysis of trends, handle outliers, and make data more suitable for certain types of analyses or machine learning algorithms\n",
    "- It involves grouping numeric values into predefined ranges, creating a categorical representation of the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "295db5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date         price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "0    2014-05-02  3.130000e+05       3.0       1.50         1340      7912   \n",
      "1    2014-05-02  2.384000e+06       5.0       2.50         3650      9050   \n",
      "2    2014-05-02  3.420000e+05       3.0       2.00         1930     11947   \n",
      "3    2014-05-02  4.200000e+05       3.0       2.25         2000      8030   \n",
      "4    2014-05-02  5.500000e+05       4.0       2.50         1940     10500   \n",
      "...         ...           ...       ...        ...          ...       ...   \n",
      "4595 2014-07-09  3.081667e+05       3.0       1.75         1510      6360   \n",
      "4596 2014-07-09  5.343333e+05       3.0       2.50         1460      7573   \n",
      "4597 2014-07-09  4.169042e+05       3.0       2.50         3010      7014   \n",
      "4598 2014-07-10  2.034000e+05       4.0       2.00         2090      6630   \n",
      "4599 2014-07-10  2.206000e+05       3.0       2.50         1490      8102   \n",
      "\n",
      "      floors  waterfront  view  condition  ...  sqft_basement  yr_built  \\\n",
      "0        1.5           0     0          3  ...              0      1955   \n",
      "1        2.0           0     4          5  ...            280      1921   \n",
      "2        1.0           0     0          4  ...              0      1966   \n",
      "3        1.0           0     0          4  ...           1000      1963   \n",
      "4        1.0           0     0          4  ...            800      1976   \n",
      "...      ...         ...   ...        ...  ...            ...       ...   \n",
      "4595     1.0           0     0          4  ...              0      1954   \n",
      "4596     2.0           0     0          3  ...              0      1983   \n",
      "4597     2.0           0     0          3  ...              0      2009   \n",
      "4598     1.0           0     0          3  ...           1020      1974   \n",
      "4599     2.0           0     0          4  ...              0      1990   \n",
      "\n",
      "      yr_renovated                    street       city  statezip country  \\\n",
      "0             2005      18810 Densmore Ave N  Shoreline  WA 98133     USA   \n",
      "1                0           709 W Blaine St    Seattle  WA 98119     USA   \n",
      "2                0  26206-26214 143rd Ave SE       Kent  WA 98042     USA   \n",
      "3                0           857 170th Pl NE   Bellevue  WA 98008     USA   \n",
      "4             1992         9105 170th Ave NE    Redmond  WA 98052     USA   \n",
      "...            ...                       ...        ...       ...     ...   \n",
      "4595          1979            501 N 143rd St    Seattle  WA 98133     USA   \n",
      "4596          2009          14855 SE 10th Pl   Bellevue  WA 98007     USA   \n",
      "4597             0          759 Ilwaco Pl NE     Renton  WA 98059     USA   \n",
      "4598             0         5148 S Creston St    Seattle  WA 98178     USA   \n",
      "4599             0         18717 SE 258th St  Covington  WA 98042     USA   \n",
      "\n",
      "      log_price  normalized_price  price_category  \n",
      "0     12.653958          0.011771            501+  \n",
      "1     14.684290          0.089658            501+  \n",
      "2     12.742566          0.012862            501+  \n",
      "3     12.948010          0.015795            501+  \n",
      "4     13.217674          0.020684            501+  \n",
      "...         ...               ...             ...  \n",
      "4595  12.638396          0.011590            501+  \n",
      "4596  13.188775          0.020095            501+  \n",
      "4597  12.940612          0.015679            501+  \n",
      "4598  12.222930          0.007649            501+  \n",
      "4599  12.304106          0.008296            501+  \n",
      "\n",
      "[4600 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data Binning: Creating bins for the 'price' column\n",
    "# checking if 'price' column exists in the dataframe\n",
    "\n",
    "if 'price' in df.columns:\n",
    "    #Define bin edges\n",
    "    bin_edges = [0,100,200,300,400,500, np.inf] #adjust bin edges as needed\n",
    "\n",
    "    #Define bin labels\n",
    "    bin_labels = ['0-100', '101-200', '201-300', '301-400', '401-500', '501+']\n",
    "\n",
    "    #creating a new categorical column 'price_category' based on binning\n",
    "    df['price_category'] = pd.cut(df['price'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "    #displaying the dataframe with the new 'price_category' column\n",
    "    print(df)\n",
    "else:\n",
    "    print('the price column does not exist in this dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb4af5",
   "metadata": {},
   "source": [
    "## Handling outliers\n",
    "- Outliers can significantly impact analysis and modeling. Identifying and addressing outliers is crucial for maintaining the accuracy of results.\n",
    "- Winssorization: it is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers by winsorizing\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Check if 'price' column exists in the dataframe\n",
    "if 'price' in df.columns:\n",
    "    #Winsorizing the 'price' column with limits [0.05, 0.05]\n",
    "    df['winsorizing_price'] = winsorize(df['price'], limits=[0.05,0.05])\n",
    "    print(df)\n",
    "else:\n",
    "    print('the column price does not exist in this dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e41c5b",
   "metadata": {},
   "source": [
    "## Pandas Joining Techniques\n",
    "- Pandas provides various joining techniques, such as merging, joining, and concatenating, which allow datasets to be combined using one or more keys. Each method has unique behaviors and applications.\n",
    "- Concatenate: It appends DataFrames vertically or horizontally, offering a straightforward way to combine datasets with distinct columns or indices without regard for overlapping keys or index values.\n",
    "- Merge: It combines DataFrames by aligning columns with shared keys, allowing for detailed control over overlapping column names and the use of multiple keys.\n",
    "- Join: It aligns DataFrames based on their index values, making it ideal for coordinating data with correspoinding indies.\n",
    "\n",
    "\n",
    "# Pandas Concatenate\n",
    "# The pd.concat() method combines DataFrames along rows or columns, preserving indices and columns.\n",
    "# Specify axis=0 to concatenate along rows(vertical concatenation) or axis=1 to concatenate along columns (horizontal concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1 = pd.DataFrame(\n",
    "   {\n",
    "       \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "       \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "       \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "       \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "   },\n",
    "   index=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "   {\n",
    "       \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n",
    "       \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n",
    "       \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n",
    "       \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n",
    "   },\n",
    "   index=[4, 5, 6, 7],\n",
    ")\n",
    "\n",
    "df3 = pd.DataFrame(\n",
    "   {\n",
    "       \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"],\n",
    "       \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"],\n",
    "       \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"],\n",
    "       \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"],\n",
    "   },\n",
    "   index=[8, 9, 10, 11],\n",
    ")\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "Result = pd.concat(frames)\n",
    "print(Result)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create two sample DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3],\n",
    "                    'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'A': [7, 8, 9],\n",
    "                    'B': [10, 11, 12]})\n",
    "\n",
    "# Concatenate along rows (stack vertically)\n",
    "Result_row = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Concatenate along columns (stack horizontally)\n",
    "Result_column = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "print(\"\\nDataframe 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataframe 2:\")\n",
    "print(df2)\n",
    "\n",
    "print(\"\\nConcatenated along rows:\")\n",
    "print(Result_row)\n",
    "\n",
    "print(\"\\nConcatenated along columns:\")\n",
    "print(Result_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7839b50",
   "metadata": {},
   "source": [
    "## Pandas Merge DataFrames\n",
    "\n",
    "- Utilize the pd.merge() method to merge DataFrames based on specific keys or columns.\n",
    "- Specify the join type in Pandas merge, which controls how rows from two DataFrames are combined.\n",
    "- This ensures data alignment and prevents unintended outcomes.\n",
    "- Choose the appropriate how parameter to specify the type of join.\n",
    "- Specify the on parameter to indicate the column(s) to merge on.\n",
    "\n",
    "## Types of Pandas Join\n",
    "\n",
    "- There are various join logics available to merge Pandas DataFrames:\n",
    "\n",
    "- Full Outer Join: It merges all rows from both DataFrames, using NaN to fill in missing values when no match is found.\n",
    "\n",
    "- Inner Join: It combines matching rows from DataFrame 1 and DataFrame 2 based on a common key column.\n",
    "\n",
    "- Right Join: It retains all rows from the right DataFrame, merges on common keys, and fills missing values with NaN.\n",
    "\n",
    "- Left Join: It retains all rows from the left DataFrame, merging matching rows from the right and filling unmatched values with NaN.\n",
    "\n",
    "- Cross: It creates the cartesian product of the rows of both frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "   {\n",
    "      \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n",
    "      \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n",
    "      \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "      \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "   }\n",
    ")\n",
    "right = pd.DataFrame(\n",
    "   {\n",
    "      \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n",
    "      \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n",
    "      \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "      \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "   }\n",
    ")\n",
    "print(\"\\nDataframe 1:\")\n",
    "print(left)\n",
    "print(\"\\nDataframe 2:\")\n",
    "print(right)\n",
    "\n",
    "#Full outer join\n",
    "Result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"])\n",
    "print(Result)\n",
    "\n",
    "#Inner join\n",
    "Result = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"])\n",
    "print(Result)\n",
    "\n",
    "#Right Join\n",
    "Result = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"])\n",
    "print(Result)\n",
    "\n",
    "#Left Join\n",
    "Result = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\n",
    "print(Result)\n",
    "\n",
    "#Cross\n",
    "Result = pd.merge(left, right, how=\"cross\")\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa344657",
   "metadata": {},
   "source": [
    "## Pandas Join DataFrame\n",
    "- Use the join() method to join DataFrames based on their indices\n",
    "- Specify the how parameter to determine the type of join, similar to pd.merge()\n",
    "- Use the on parameter if joining on specific columns, or simply call join() without parameters to perform a simple index-based join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "        \"key\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n",
    "    }\n",
    ")\n",
    "right = pd.DataFrame({\"C\": [\"C0\", \"C1\"],\n",
    "                      \"D\": [\"D0\", \"D1\"]},\n",
    "                      index=[\"K0\", \"K1\"])\n",
    "\n",
    "Result = left.join(right, on=\"key\")\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c0185",
   "metadata": {},
   "source": [
    "## Aggregating Data\n",
    "- Aggregating data involves summarizing or grouping data based on specific criteria. This is useful for creating meaningful insights and reducing data dimensionality.\n",
    "- Common aggregation functions include average(mean), median, minimum(min), maximum(man), sum, standard deviation(std), variance(var), and count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame with a 'Category' column and a 'Value' column\n",
    "data = {'Category': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'Value': [10, 15, 20, 25, 30]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Grouping the DataFrame by 'Category' and calculating various aggregations\n",
    "df_aggregated = df.groupby('Category').agg({\n",
    "    'Value': ['mean', 'median', 'min', 'max', 'sum', 'std', 'var', 'count']\n",
    "})\n",
    "\n",
    "# Displaying the aggregated DataFrame\n",
    "print(\"Aggregated DataFrame:\")\n",
    "df_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83760d8",
   "metadata": {},
   "source": [
    "## Reshaping Data\n",
    "- Reshaping data includes pivoting, melting, or stacking data to achieve a structure suitable for specific analyses or visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with 'Date', 'Category', and 'Value' columns\n",
    "# Adjust column names and DataFrame based on your actual data\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame({'Date': ['2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02'],\n",
    "                   'Category': ['A', 'B', 'A', 'B'],\n",
    "                   'Value': [10, 15, 20, 25]})\n",
    "\n",
    "# Pivoting data for better analysis\n",
    "df_pivoted = df.pivot_table(index='Date', columns='Category', values='Value', aggfunc='mean')\n",
    "\n",
    "# Displaying the pivoted DataFrame\n",
    "print(\"Pivoted DataFrame:\")\n",
    "print(df_pivoted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
